{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indirect-nation",
   "metadata": {},
   "source": [
    "28丨EM聚类（上）：如何将一份菜等分给两个人？\n",
    "====\n",
    "今天我来带你学习 EM 聚类。EM 的英文是 Expectation Maximization，所以 EM 算法也叫最大期望算法。\n",
    "\n",
    "我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？\n",
    "\n",
    "很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子 A 中，然后再把剩余的分到碟子 B 中，再来观察碟子 A 和 B 里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子 A 和 B 里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。\n",
    "\n",
    "你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是 EM 算法的过程。\n",
    "<img src=\"./images/28-01.jpg\">\n",
    "\n",
    "EM 算法的工作原理\n",
    "====\n",
    "说到 EM 算法，我们先来看一个概念“最大似然”，英文是 Maximum Likelihood，Likelihood 代表可能性，所以最大似然也就是最大可能性的意思。\n",
    "\n",
    "什么是最大似然呢？举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是**最大似然的概念**。\n",
    "\n",
    "最大似然估计是什么呢？它指的就是一件事情已经发生了，然后反推更有**可能是什么因素造成的**。还是用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性。**最大似然估计是一种通过已知结果，估计参数的方法。**\n",
    "\n",
    "那么 EM 算法是什么？它和最大似然估计又有什么关系呢？EM 算法是一种**求解最大似然估计的方法**，通过观测样本，来找出样本的模型参数。\n",
    "\n",
    "再回过来看下开头我给你举的分菜的这个例子，实际上最终我们想要的是碟子 A 和碟子 B 中菜的份量，你可以把它们理解为想要求得的**模型参数**。然后我们通过 EM 算法中的 E 步来进行观察，然后通过 M 步来进行调整 A 和 B 的参数，最后让碟子 A 和碟子 B 的参数不再发生变化为止。\n",
    "\n",
    "实际我们遇到的问题，比分菜复杂。我再给你举个一个投掷硬币的例子，假设我们有 A 和 B 两枚硬币，我们做了 5 组实验，每组实验投掷 10 次，然后统计出现正面的次数，实验结果如下：\n",
    "<img src=\"./images/28-02.png\">\n",
    "\n",
    "投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是 A 还是 B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：\n",
    "<img src=\"./images/28-03.png\">\n",
    "\n",
    "我们现在想要求得硬币 A 和 B 出现正面次数的概率，可以直接求得：\n",
    "<img src=\"./images/28-04.png\">\n",
    "\n",
    "**而实际情况是我不知道每次投掷的硬币是 A 还是 B**，那么如何求得硬币 A 和硬币 B 出现正面的概率呢？\n",
    "\n",
    "这里就需要采用 EM 算法的思想。\n",
    "1. 初始化参数。我们假设硬币 A 和 B 的正面概率（随机指定）是θA=0.5 和θB=0.9。2. \n",
    "2. 计算期望值。假设实验 1 投掷的是硬币 A，那么正面次数为 5 的概率为：\n",
    "<img src=\"./images/28-05.png\">\n",
    "\n",
    "    公式中的 C(10,5) 代表的是 10 个里面取 5 个的组合方式，也就是排列组合公式，0.5 的 5 次方乘以 0.5 的 5 次方代表的是其中一次为 5 次为正面，5 次为反面的概率，然后再乘以 C(10,5) 等于正面次数为 5 的概率。\n",
    "\n",
    "    假设实验 1 是投掷的硬币 B ，那么正面次数为 5 的概率为：\n",
    "    <img src=\"./images/28-06.png\">\n",
    "    所以实验 1 更有可能投掷的是硬币 A。\n",
    "\n",
    "    然后我们对实验 2~5 重复上面的计算过程，可以推理出来硬币顺序应该是{A，A，B，B，A}。\n",
    "\n",
    "    这个过程实际上是通过假设的参数来估计未知参数，即“每次投掷是哪枚硬币”。\n",
    "    \n",
    "3. 通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。\n",
    "\n",
    "    然后一直重复第二步和第三步，直到参数不再发生变化。\n",
    "\n",
    "简单总结下上面的步骤，你能看出 EM 算法中的 E 步骤就是通过**旧的参数来计算隐藏变量**。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。\n",
    "\n",
    "EM 聚类的工作原理\n",
    "====\n",
    "上面你能看到 EM 算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是 EM 聚类的原理。\n",
    "\n",
    "相比于 K-Means 算法，EM 聚类更加灵活，比如下面这两种情况，K-Means 会得到下面的聚类结果。\n",
    "<img src=\"./images/28-07.jpg\">\n",
    "\n",
    "因为 K-Means 是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是**硬聚类算法**。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做**软聚类算法**。\n",
    "\n",
    "你可以把 EM 算法理解成为是一个**框架**，在这个框架中可以采用不同的模型来用 EM 进行求解。常用的 EM 聚类有 **GMM 高斯混合模型**和 **HMM 隐马尔科夫模型**。GMM（高斯混合模型）聚类就是 EM 聚类的一种。比如上面这两个图，可以采用 GMM 来进行聚类。\n",
    "\n",
    "和 K-Means 一样，我们事先知道聚**类的个数**，但是不知道每个样本分别属于哪一类。通常，我们可以假设样本是符合**高斯分布的（也就是正态分布）**。每个高斯分布都属于这个模型的组成部分（component），要分成 K 类就相当于是 K 个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是 E 步骤。\n",
    "\n",
    "再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即 M 步骤。反复 EM 步骤，直到每个组成部分的高斯分布参数不变为止。\n",
    "\n",
    "这样也就相当于将样本按照 GMM 模型进行了 EM 聚类。\n",
    "<img src=\"./images/28-08.jpg\">\n",
    "\n",
    "总结\n",
    "====\n",
    "EM 算法相当于一个框架，你可以采用不同的模型来进行聚类，比如 GMM（高斯混合模型），或者 HMM（隐马尔科夫模型）来进行聚类。GMM 是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而 HMM 用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM 在自然语言处理和语音识别领域中有广泛的应用。\n",
    "\n",
    "在 EM 这个框架中，E 步骤相当于是通过初始化的参数来估计隐含变量。M 步骤就是通过隐含变量反推来优化参数。最后通过 EM 步骤的迭代得到模型参数。\n",
    "\n",
    "在这个过程里用到的一些数学公式这节课不进行展开。你需要重点理解 EM 算法的原理。通过上面举的炒菜的例子，你可以知道 EM 算法是一个**不断观察和调整**的过程。\n",
    "\n",
    "通过求硬币正面概率的例子，你可以理解如何通过初始化参数来求隐含数据的过程，以及再通过求得的隐含数据来优化参数。\n",
    "\n",
    "通过上面 GMM 图像聚类的例子，你可以知道很多 K-Means 解决不了的问题，EM 聚类是可以解决的。在 EM 框架中，我们将**潜在类别当做隐藏变量**，样本看做观察值，把**聚类问题**转化为**参数估计问题**，最终把样本进行聚类。\n",
    "<img src=\"./images/28-09.png\">\n",
    "\n",
    "思考题\n",
    "====\n",
    "1. 你能用自己的话说一下 EM 算法的原理吗？\n",
    "2. EM 聚类和 K-Means 聚类的相同和不同之处又有哪些？\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-dinner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
