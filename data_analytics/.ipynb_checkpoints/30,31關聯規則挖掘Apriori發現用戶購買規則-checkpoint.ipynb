{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-monroe",
   "metadata": {},
   "source": [
    "关联规则挖掘可以让我们从数据集中发现项与项（item 与 item）之间的关系，它在我们的生活中有很多应用场景，“购物篮分析”就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。\n",
    "\n",
    "在今天的内容中，希望你能带着问题，和我一起来搞懂以下几个知识点：\n",
    "1. 搞懂关联规则中的几个重要概念：支持度、置信度、提升度；\n",
    "2. Apriori 算法的工作原理；\n",
    "3. 在实际工作中，我们该如何进行关联规则挖掘。\n",
    "\n",
    "搞懂关联规则中的几个概念\n",
    "====\n",
    "我举一个超市购物的例子，下面是几名客户购买的商品列表：\n",
    "<img src=\"./images/30-01.png\">\n",
    "## 什么是支持度呢？\n",
    "支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。\n",
    "\n",
    "在这个例子中，我们能看到“牛奶”出现了 4 次，那么这 5 笔订单中“牛奶”的支持度就是 4/5=0.8。\n",
    "\n",
    "同样“牛奶 + 面包”出现了 3 次，那么这 5 笔订单中“牛奶 + 面包”的支持度就是 3/5=0.6。\n",
    "## 什么是置信度呢？\n",
    "它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：\n",
    "* 置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？\n",
    "* 置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？\n",
    "\n",
    "我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度 (牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2 次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。\n",
    "\n",
    "### <font color=#800000>所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。</font>\n",
    "## 什么是提升度呢？\n",
    "我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是<font color=#800000>“商品 A 的出现，对商品 B 的出现概率提升的”程度</font>。\n",
    "\n",
    "还是看上面的例子，如果我们单纯看置信度 (可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？\n",
    "\n",
    "实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品 A 对商品 B 的提升度：\n",
    "\n",
    "提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)\n",
    "\n",
    "这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。所以提升度有三种可能：\n",
    "1. 提升度 (A→B)>1：代表有提升；\n",
    "2. 提升度 (A→B)=1：代表有没有提升，也没有下降；\n",
    "3. 提升度 (A→B)<1：代表有下降。\n",
    "\n",
    "## Apriori 的工作原理\n",
    "明白了关联规则中支持度、置信度和提升度这几个重要概念，我们来看下 Apriori 算法是如何工作的。\n",
    "\n",
    "首先我们把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：\n",
    "<img src=\"./images/30-02.png\">\n",
    "Apriori 算法其实就是查找频繁项集 (frequent itemset) 的过程，所以首先我们需要定义什么是频繁项集。\n",
    "\n",
    "频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。\n",
    "\n",
    "项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。我们再来看下这个例子，假设我随机指定最小支持度是 50%，也就是 0.5。\n",
    "\n",
    "我们来看下 Apriori 算法是如何运算的。\n",
    "\n",
    "首先，我们先计算单个商品的支持度，也就是得到 K=1 项的支持度：\n",
    "<img src=\"./images/30-03.png\">\n",
    "\n",
    "因为最小支持度是 0.5，所以你能看到商品 4、6 是不符合最小支持度的，**不属于频繁项集**，于是经过筛选商品的频繁项集就变成：\n",
    "<img src=\"./images/30-04.png\">\n",
    "\n",
    "在这个基础上，我们将商品两两组合，得到 k=2 项的支持度：\n",
    "<img src=\"./images/30-05.png\">\n",
    "\n",
    "我们再筛掉小于最小值支持度的商品组合，可以得到：\n",
    "\n",
    "|商品項集|支持度|\n",
    "|----|----|\n",
    "|1,2|3/5|\n",
    "|2,3|4/5|\n",
    "|3,5|3/5|\n",
    "\n",
    "我们再将商品进行 K=3 项的商品组合，可以得到：\n",
    "\n",
    "|商品項集|支持度|\n",
    "|----|----|\n",
    "|1,2,3|3/5|\n",
    "|2,3,5|2/5|\n",
    "|1,2,5|1/5|\n",
    "\n",
    "再筛掉小于最小值支持度的商品组合，可以得到：\n",
    "\n",
    "|商品項集|支持度|\n",
    "|----|----|\n",
    "|1,2,3|3/5|\n",
    "\n",
    "通过上面这个过程，我们可以得到 K=3 项的频繁项集{1,2,3}，也就是{牛奶、面包、尿布}的组合。\n",
    "\n",
    "到这里，你已经和我模拟了一遍整个 Apriori 算法的流程，下面我来给你总结下 Apriori 算法的递归流程：\n",
    "1. K=1，计算 K 项集的支持度；\n",
    "2. 筛选掉小于最小支持度的项集；\n",
    "3. 如果项集为空，则对应 K-1 项集的结果为最终结果。\n",
    "\n",
    "否则 K=K+1，重复 1-3 步。\n",
    "\n",
    "## Apriori 的改进算法：FP-Growth 算法\n",
    "我们刚完成了 Apriori 算法的模拟，你能看到 Apriori 在计算的过程中有以下几个缺点：\n",
    "1. 可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；\n",
    "2. 每次计算都需要重新扫描数据集，来计算每个项集的支持度。\n",
    "\n",
    "所以 **Apriori 算法会浪费很多计算空间和计算时间**，为此人们提出了 FP-Growth 算法，它的特点是：\n",
    "1. 创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；\n",
    "2. 整个生成过程只遍历数据集 2 次，大大减少了计算量。\n",
    "\n",
    "所以在实际工作中，我们常用 FP-Growth 来做频繁项集的挖掘，下面我给你简述下 FP-Growth 的原理。\n",
    "\n",
    "### 1. 创建项头表（item header table）\n",
    "创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。\n",
    "\n",
    "这一步的流程是先扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。\n",
    "\n",
    "项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。\n",
    "\n",
    "|項|支持度|鏈表|\n",
    "|----|----|----|\n",
    "|尿布|5||\n",
    "|牛奶|4||\n",
    "|麵包|4||\n",
    "|啤酒|3||\n",
    "\n",
    "### 2. 構造FP樹\n",
    "FP 树的根节点记为 NULL 节点。\n",
    "\n",
    "整个流程是需要再次扫描数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。\n",
    "<img src=\"30-07.png\">\n",
    "\n",
    "### 3. 通过 FP 树挖掘频繁项集\n",
    "到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出每个频繁项集。\n",
    "\n",
    "具体的操作会用到一个概念，叫**条件模式基**，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的**祖先节点设置为叶子节点之和**。\n",
    "\n",
    "我以“啤酒”的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：\n",
    "<img src=\"./images/30.08.png\">\n",
    "你能看出来，相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以“啤酒”为节点的 FP 子树，也就是说，在频繁项集中一定要含有“啤酒”这个项。你可以再看下原始的数据，其中订单 1{牛奶、面包、尿布}和订单 5{牛奶、面包、尿布、可乐}并不存在“啤酒”这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以“啤酒”为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。\n",
    "\n",
    "**条件模式基**不包括“啤酒”节点，而且祖先节点如果小于**最小支持度**就会被剪枝，所以“啤酒”的**条件模式基为空**。\n",
    "\n",
    "同理，我们可以求得“面包”的条件模式基为：\n",
    "<img src=\"./images/30-09.png\">\n",
    "所以可以求得面包的频繁项集为{尿布，面包}，{尿布，牛奶，面包}。同样，我们还可以求得牛奶，尿布的频繁项集，这里就不再计算展示。\n",
    "\n",
    "总结\n",
    "====\n",
    "今天我给你讲了 Apriori 算法，它是在“购物篮分析”中常用的关联规则挖掘算法，在 Apriori 算法中你最主要是需要明白支持度、置信度、提升度这几个概念，以及 Apriori 迭代计算频繁项集的工作流程。\n",
    "\n",
    "Apriori 算法在实际工作中需要对数据集扫描多次，会消耗大量的计算时间，所以在 2000 年 FP-Growth 算法被提出来，它只需要扫描两次数据集即可以完成关联规则的挖掘。FP-Growth 算法最主要的贡献就是提出了 **FP 树和项头表，通过 FP 树减少了频繁项集的存储以及计算时间**。\n",
    "\n",
    "当然 Apriori 的改进算法除了 FP-Growth 算法以外，还有 CBA 算法、GSP 算法，这里就不进行介绍。\n",
    "\n",
    "你能发现一种新理论的提出，往往是先从最原始的概念出发，提出一种新的方法。原始概念最接近人们模拟的过程，但往往会存在空间和时间复杂度过高的情况。所以后面其他人会对这个方法做改进型的创新，重点是在**空间和时间复杂度上进行降维**，比如采用新型的数据结构。你能看出树在**存储和检索**中是一个非常好用的数据结构。\n",
    "<img src=\"./images/30-10.png\">\n",
    "\n",
    "思考题\n",
    "====\n",
    "1. 你能说一说 Apriori 的工作原理吗？\n",
    "2. 相比于 Apriori，FP-Growth 算法都有哪些改进？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-plastic",
   "metadata": {},
   "source": [
    "31丨关联规则挖掘（下）：导演如何选择演员？\n",
    "====\n",
    "上次我给你讲了关联规则挖掘的原理。关联规则挖掘在生活中有很多使用场景，不仅是商品的捆绑销售，甚至在挑选演员决策上，你也能通过关联规则挖掘看出来某个导演选择演员的倾向。\n",
    "\n",
    "今天我来带你用 Apriori 算法做一个项目实战。你需要掌握的是以下几点：\n",
    "1. 熟悉上节课讲到的几个重要概念：支持度、置信度和提升度；\n",
    "2. 熟悉与掌握 Apriori 工具包的使用；\n",
    "3. 在实际问题中，灵活运用。包括数据集的准备等。\n",
    "\n",
    "如何使用 Apriori 工具包\n",
    "====\n",
    "Apriori 虽然是十大算法之一，不过在 sklearn 工具包中并没有它，也没有 FP-Growth 算法。这里教你个方法，来选择 Python 中可以使用的工具包，你可以通过https://pypi.org/ 搜索工具包。\n",
    "<img src=\"./images/31-01.png\">\n",
    "\n",
    "这个网站提供的工具包都是 Python 语言的，你能找到 8 个 Python 语言的 Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即 efficient-apriori。后面我会讲到为什么推荐这个工具包。\n",
    "\n",
    "首先你需要通过 pip install efficient-apriori 安装这个工具包。\n",
    "\n",
    "然后看下如何使用它，核心的代码就是这一行：\n",
    "```python\n",
    "itemsets, rules = apriori(data, min_support,  min_confidence)\n",
    "```\n",
    "1. 其中 data 是我们要提供的数据集，它是一个 list 数组类型。\n",
    "2. min_support 参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。\n",
    "    * 支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的概率越大\n",
    "3. min_confidence 是最小置信度，数值也代表百分比，比如 1 代表 100%。\n",
    "    * 置信度是一个条件概念，就是在 A 发生的情况下，B 发生的概率是多少。\n",
    "    * 提升度代表的是**“商品 A 的出现，对商品 B 的出现概率提升了多少”。**\n",
    "\n",
    "接下来我们用这个工具包，跑一下上节课中讲到的超市购物的例子。下面是客户购买的商品列表：\n",
    "<img src=\"./images/31-02.png\">\n",
    "具体实现的代码如下：\n",
    "```python\n",
    "from efficient_apriori import apriori\n",
    "# 设置数据集\n",
    "data = [('牛奶','面包','尿布'),\n",
    "           ('可乐','面包', '尿布', '啤酒'),\n",
    "           ('牛奶','尿布', '啤酒', '鸡蛋'),\n",
    "           ('面包', '牛奶', '尿布', '啤酒'),\n",
    "           ('面包', '牛奶', '尿布', '可乐')]\n",
    "# 挖掘频繁项集和频繁规则\n",
    "itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)\n",
    "print(itemsets)\n",
    "print(rules)\n",
    "```\n",
    "运行结果：\n",
    "```python\n",
    "{1: {('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 4}, 2: {('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3}, 3: {('尿布', '牛奶', '面包'): 3}}\n",
    "[{啤酒} -> {尿布}, {牛奶} -> {尿布}, {面包} -> {尿布}, {牛奶, 面包} -> {尿布}]\n",
    "```\n",
    "你能从代码中看出来，data 是个 List 数组类型，其中每个值都可以是一个集合。实际上你也可以把 data 数组中的每个值设置为 List 数组类型，比如：\n",
    "```python\n",
    "data = [['牛奶','面包','尿布'],\n",
    "           ['可乐','面包', '尿布', '啤酒'],\n",
    "           ['牛奶','尿布', '啤酒', '鸡蛋'],\n",
    "           ['面包', '牛奶', '尿布', '啤酒'],\n",
    "           ['面包', '牛奶', '尿布', '可乐']]\n",
    "```\n",
    "两者的运行结果是一样的，efficient-apriori 工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。\n",
    "\n",
    "而其他的 Apriori 算法可能会**因为考虑了先后顺序**，出现计算频繁项集结果不对的情况。所以这里采用的是 efficient-apriori 这个工具包。\n",
    "\n",
    "挖掘導演是如何選擇演員\n",
    "====\n",
    "在实际工作中，数据集是需要自己来准备的，比如今天我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的 Python 爬虫进行数据采集。\n",
    "\n",
    "不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。\n",
    "\n",
    "先来梳理下采集的工作流程。\n",
    "\n",
    "首先我们先在https://movie.douban.com搜索框中输入导演姓名，比如“宁浩”。\n",
    "\n",
    "页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：\n",
    "1. 页面默认是 15 条数据反馈，第一页会返回 16 条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。\n",
    "2. 每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用“/”分割。\n",
    "\n",
    "有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在 csv 文件中。完整的抓取代码如下：\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "# 下载某个导演的电影数据集\n",
    "from efficient_apriori import apriori\n",
    "from lxml import etree\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "driver = webdriver.Chrome()\n",
    "# 设置想要下载的导演 数据集\n",
    "director = u'宁浩'\n",
    "# 写CSV文件\n",
    "file_name = './' + director + '.csv'\n",
    "base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&cat=1002&start='\n",
    "out = open(file_name,'w', newline='', encoding='utf-8-sig')\n",
    "csv_write = csv.writer(out, dialect='excel')\n",
    "flags=[]\n",
    "# 下载指定页面的数据\n",
    "def download(request_url):\n",
    "  driver.get(request_url)\n",
    "  time.sleep(1)\n",
    "  html = driver.find_element_by_xpath(\"//*\").get_attribute(\"outerHTML\")\n",
    "  html = etree.HTML(html)\n",
    "  # 设置电影名称，导演演员 的XPATH\n",
    "  movie_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']\")\n",
    "  name_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']\")\n",
    "  # 获取返回的数据个数\n",
    "  num = len(movie_lists)\n",
    "  if num > 15: #第一页会有16条数据\n",
    "    # 默认第一个不是，所以需要去掉\n",
    "    movie_lists = movie_lists[1:]\n",
    "    name_lists = name_lists[1:]\n",
    "  for (movie, name_list) in zip(movie_lists, name_lists):\n",
    "    # 会存在数据为空的情况\n",
    "    if name_list.text is None: \n",
    "      continue\n",
    "    # 显示下演员名称\n",
    "    print(name_list.text)\n",
    "    names = name_list.text.split('/')\n",
    "    # 判断导演是否为指定的director\n",
    "    if names[0].strip() == director and movie.text not in flags:\n",
    "      # 将第一个字段设置为电影名称\n",
    "      names[0] = movie.text\n",
    "      flags.append(movie.text)\n",
    "      csv_write.writerow(names)\n",
    "  print('OK') # 代表这页数据下载成功\n",
    "  print(num)\n",
    "  if num >= 14: #有可能一页会有14个电影\n",
    "    # 继续下一页\n",
    "    return True\n",
    "  else:\n",
    "    # 没有下一页\n",
    "    return False\n",
    "\n",
    "# 开始的ID为0，每页增加15\n",
    "start = 0\n",
    "while start<10000: #最多抽取1万部电影\n",
    "  request_url = base_url + str(start)\n",
    "  # 下载数据，并返回是否有下一页\n",
    "  flag = download(request_url)\n",
    "  if flag:\n",
    "    start = start + 15\n",
    "  else:\n",
    "    break\n",
    "out.close()\n",
    "print('finished')\n",
    "```\n",
    "代码中涉及到了几个模块，我简单讲解下这几个模块。\n",
    "\n",
    "在引用包这一段，我们使用 csv 工具包读写 CSV 文件，用 efficient_apriori 完成 Apriori 算法，用 lxml 进行 XPath 解析，time 工具包可以让我们在模拟后有个适当停留，代码中我设置为 1 秒钟，等 HTML 数据完全返回后再进行 HTML 内容的获取。使用 selenium 的 webdriver 来模拟浏览器的行为。\n",
    "\n",
    "在读写文件这一块，我们需要事先告诉 python 的 open 函数，文件的编码是 utf-8-sig（对应代码：encoding=‘utf-8-sig’），这是因为我们会用到中文，为了避免编码混乱。\n",
    "\n",
    "编写 download 函数，参数传入我们要采集的页面地址（request_url）。针对返回的 HTML，我们需要用到之前讲到的 Chrome 浏览器的 XPath Helper 工具，来获取电影名称以及演出人员的 XPath。我用页面返回的数据个数来判断当前所处的页面序号。如果数据个数 >15，也就是第一页，第一页的第一条数据是广告，我们需要忽略。如果数据个数 =15，代表是中间页，需要点击“下一页”，也就是翻页。如果数据个数 <15，代表最后一页，没有下一页。\n",
    "\n",
    "在程序主体部分，我们设置 start 代表抓取的 ID，从 0 开始最多抓取 1 万部电影的数据（一个导演不会超过 1 万部电影），每次翻页 start 自动增加 15，直到 flag=False 为止，也就是不存在下一页的情况。\n",
    "\n",
    "你可以模拟下抓取的流程，获得指定导演的数据，比如我上面抓取的宁浩的数据。这里需要注意的是，豆瓣的电影数据可能是不全的，但基本上够我们用。\n",
    "<img src=\"./images/31-03.png\">\n",
    "有了数据之后，我们就可以用 Apriori 算法来挖掘频繁项集和关联规则，代码如下：\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "from efficient_apriori import apriori\n",
    "import csv\n",
    "director = u'宁浩'\n",
    "file_name = './'+director+'.csv'\n",
    "lists = csv.reader(open(file_name, 'r', encoding='utf-8-sig'))\n",
    "# 数据加载\n",
    "data = []\n",
    "for names in lists:\n",
    "     name_new = []\n",
    "     for name in names:\n",
    "           # 去掉演员数据中的空格\n",
    "           name_new.append(name.strip())\n",
    "     data.append(name_new[1:])\n",
    "# 挖掘频繁项集和关联规则\n",
    "itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)\n",
    "print(itemsets)\n",
    "print(rules)\n",
    "```\n",
    "代码中使用的 apriori 方法和开头中用 Apriori 获取购物篮规律的方法类似，比如代码中都设定了最小支持度和最小置信系数，这样我们可以找到支持度大于 50%，置信系数为 1 的频繁项集和关联规则。\n",
    "\n",
    "这是最后的运行结果：\n",
    "```python\n",
    "{1: {('徐峥',): 5, ('黄渤',): 6}, 2: {('徐峥', '黄渤'): 5}}\n",
    "[{徐峥} -> {黄渤}]\n",
    "```\n",
    "你能看出来，宁浩导演喜欢用徐峥和黄渤，并且有徐峥的情况下，一般都会用黄渤。你也可以用上面的代码来挖掘下其他导演选择演员的规律。\n",
    "\n",
    "總結\n",
    "====\n",
    "总结Apriori 算法的核心就是理解频繁项集和关联规则。在算法运算的过程中，还要重点掌握对支持度、置信度和提升度的理解。在工具使用上，你可以使用 efficient-apriori 这个工具包，它会把每一条数据中的项（item）放到一个集合（篮子）里来处理，不考虑项（item）之间的先后顺序。\n",
    "\n",
    "在实际运用中你还需要灵活处理，比如导演如何选择演员这个案例，虽然工具的使用会很方便，但重要的还是数据挖掘前的准备过程，也就是获取某个导演的电影数据集。\n",
    "<img src=\"./images/31-04.png\">\n",
    "\n",
    "思考题\n",
    "====\n",
    "请你编写代码挖掘下张艺谋导演使用演员的频繁项集和关联规则，最小支持度可以设置为 0.1 或 0.05。另外你认为 Apriori 算法中的最小支持度和最小置信度，一般设置为多少比较合理？\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-baptist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
