{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prompt-violence",
   "metadata": {},
   "source": [
    "17 丨决策树（上）：要不要去打篮球？决策树来告诉你\n",
    "====\n",
    "\n",
    "做决策树的时候，会经历两个阶段：**构造和剪枝**。\n",
    "\n",
    "### 构造 就是生成一棵完整的决策树。\n",
    "在构造过程中，会存在三种节点：\n",
    "1. 根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；\n",
    "2. 内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；\n",
    "3. 叶节点：就是树最底部的节点，也就是决策结果。\n",
    "\n",
    "你要解决三个重要的问题：\n",
    "1. 选择哪个属性作为根节点；\n",
    "2. 选择哪些属性作为子节点；\n",
    "3. 什么时候停止并得到目标状态，即叶节点。\n",
    "\n",
    "<img src=\"./images/17-01.jpg\">\n",
    "\n",
    "### 剪枝 \n",
    "1. 防止 **“过拟合”（Overfitting）现象**的发生。模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。\n",
    "\n",
    "<img src=\"./images/17-02.jpg\">\n",
    "\n",
    "造成过拟合的原因之一就是因为**训练集中样本量较小**。如果决策树选择的**属性过多**，构造出来的决策树一定能够“完美”地把训练集中的**样本分类**，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是**模型的“泛化能力”差**。\n",
    "\n",
    "2. **泛化能力**指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。\n",
    "\n",
    "如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。\n",
    "\n",
    "既然要对决策树进行剪枝，具体有哪些方法呢？一般来说，剪枝可以分为**预剪枝（Pre-Pruning）和“后剪枝”（Post-Pruning）**。\n",
    "\n",
    "1. 预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在**验证集**中不能带来**准确性的提升**，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。\n",
    "2. 后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在**分类准确性上差别不大**，或者剪掉该节点子树，能在验证集中带来**准确性的提升**，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。\n",
    "\n",
    "<img src=\"./images/17-03.png\">\n",
    "\n",
    "显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：**纯度和信息熵**。\n",
    "\n",
    "### 純度，目標變量的分歧最小：\n",
    "我在这里举个例子，假设有 3 个集合：\n",
    "1. 集合 1：6 次都去打篮球；純度最高，分歧最小\n",
    "2. 集合 2：4 次去打篮球，2 次不去打篮球；\n",
    "3. 集合 3：3 次去打篮球，3 次不去打篮球。\n",
    "按照纯度指标来说，集合 1> 集合 2> 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。\n",
    "\n",
    "### 信息熵(entropy)，表示訊息的不確定度：\n",
    "信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：\n",
    "<img src=\"./images/17-04.png\">\n",
    "**当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。**\n",
    "我举个简单的例子，假设有 2 个集合\n",
    "1. 集合 1：5 次去打篮球，1 次不去打篮球；\n",
    "2. 集合 2：3 次去打篮球，3 次不去打篮球。\n",
    "\n",
    "計算兩個類別(打籃球、不打籃球)的概率：\n",
    "\n",
    "|集合|打籃球|不打籃球|打籃球概率|不打籃球概率|\n",
    "|----|----|----|----|----|\n",
    "|1|5|1|5/6|1/6|\n",
    "|2|3|3|3/6|3/6|\n",
    "\n",
    "將兩個集合分別帶入訊息熵的公式：\n",
    "#### 集合1 訊息熵為0.65較低，純度較高。\n",
    "<img src=\"./images/17-05.png\">\n",
    "\n",
    "#### 集合2 訊息熵為1較高，純度較低。\n",
    "<img src=\"./images/17-06.png\">\n",
    "\n",
    "\n",
    "### 经典的 “不纯度”的指标有三种，\n",
    "构造决策树的时候，会基于**纯度来构建**分别是:\n",
    "1. 信息增益（ID3 算法）\n",
    "2. 信息增益率（C4.5 算法）\n",
    "3. 基尼指数（Cart 算法）。\n",
    "\n",
    "### 1. 信息增益（ID3 算法）\n",
    "**指的就是划分可以带来纯度的提高，信息熵的下降。**\n",
    "\n",
    "公式：**父亲节点的信息熵**减去**所有子节点的信息熵**。公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。\n",
    "<img src=\"./images/17-07.png\">\n",
    "\n",
    "假设(天气 = 晴)的时候，会有 5 次去打篮球，5 次不打篮球。其中 \n",
    "1. D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。\n",
    "2. D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。\n",
    "\n",
    "那么 **a 代表节点的属性，即天气 = 晴**。\n",
    "<img src=\"./images/17-08.jpg\">\n",
    "\n",
    "比如针对图上这个例子，D 作为节点的信息增益为：\n",
    "<img src=\"./images/17-09.png\">\n",
    "\n",
    "也就是 (D 节点的信息熵)－(2 个子节点的归一化信息熵)。2 个子节点归一化信息熵 =(3/10 的 D1 信息熵 +7/10 的 D2 信息熵)。\n",
    "<img src=\"./images/17-10.png\">\n",
    "\n",
    "如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，于是我们可以用下面的方式来记录 D1，D2，D3：\n",
    "1. D1(天气 = 晴天)={1-,2-,6+}\n",
    "2. D2(天气 = 阴天)={3+,7-}\n",
    "3. D3(天气 = 小雨)={4+,5-}\n",
    "\n",
    "我们先分别计算三个叶子节点的信息熵：\n",
    "<img src=\"./images/17-11.png\">\n",
    "\n",
    "因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（父节点）中的概率是 3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是 2/7。\n",
    "\n",
    "那么作为子节点的归一化信息熵 = 3/7\\*0.918+2/7\\*1.0+2/7\\*1.0=0.965。\n",
    "\n",
    "同理我们可以计算出其他属性作为根节点的信息增益，它们分别为 ：\n",
    "1. Gain(D , 温度)=0.128\n",
    "2. Gain(D , 湿度)=0.020\n",
    "3. Gain(D , 刮风)=0.020\n",
    "\n",
    "我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：\n",
    "<img src=\"./images/17-12.jpg\">\n",
    "\n",
    "然后我们要将上图中第一个叶节点，也就是 D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：\n",
    "1. Gain(D , 湿度)=1\n",
    "2. Gain(D , 天气)=1\n",
    "3. Gain(D , 刮风)=0.3115\n",
    "\n",
    "我们能看到湿度，或者天气为 D1 的节点都可以得到最大的信息增益，这里我们选取湿度作为节点的属性划分。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下：\n",
    "<img src=\"./images/17-13.jpg\">\n",
    "\n",
    "ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，ID3 算法倾向于选择取值比较多的属性。这样，如果我们把**编号**作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。\n",
    "\n",
    "所以 ID3 有一个缺陷就是，**有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性**。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-louisiana",
   "metadata": {},
   "source": [
    "在 ID3 算法上进行改进的 C4.5 算法\n",
    "====\n",
    "\n",
    "#### 1. 采用信息增益率\n",
    "* 因为 ID3 在计算的时候，倾向于**选择取值多的属性**。为了避免这个问题，C4.5 采用**信息增益率**的方式来选择属性。**信息增益率 = 信息增益 / 属性熵**，具体的计算公式这里省略。\n",
    "* 当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，**属性熵**也会变大，所以整体的信息增益率并不大。\n",
    "    \n",
    "#### 2. 采用悲观剪枝\n",
    "* ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。\n",
    "* 悲观剪枝是后剪枝技术中的一种，通过**递归**估算每个内部节点的**分类错误率**，比较剪枝前后这个节点的**分类错误率来决定是否对其进行剪枝**。这种剪枝方法不再需要一个**单独的测试数据集**。\n",
    "    \n",
    "#### 3. 离散化处理连续属性\n",
    "* C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。\n",
    "* 比如打篮球存在的“湿度”属性，**不按照“高、中”划分**，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个**阈值**呢，C4.5 选择具有**最高信息增益**的划分所对应的阈值。\n",
    "\n",
    "#### 4. 处理缺失值\n",
    "* 针对数据集不完整的情况，C4.5 也可以进行处理。\n",
    "* 假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。\n",
    "    1. 第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？\n",
    "    2. 第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？\n",
    "    \n",
    "<img src=\"./images/17-14.png\">\n",
    "\n",
    "* 我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。\n",
    "* 温度 = 高：D1={2-,3+,4+} ；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。\n",
    "* 这里 + 号代表打篮球，- 号代表不打篮球。比如 ID=2 时，决策是不打篮球，我们可以记录为 2-。\n",
    "\n",
    "针对将属性选择为温度的信息增为，这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。\n",
    "- Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208\n",
    "- 属性熵 =1.459, 信息增益率 Gain_ratio(D′, 温度)=0.208/1.459=0.1426。\n",
    "\n",
    "#### 兩者比較：\n",
    "* 首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。\n",
    "* C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。\n",
    "\n",
    "### 總結\n",
    "Python 的 sklearn，或者是 Weka（一个免费的数据挖掘工作平台）已经集成了这两种算法。只是我们在了解了这两种算法之后，才能更加清楚这两种算法的优缺点。\n",
    "\n",
    "<img src=\"./images/17-15.png\">\n",
    "\n",
    "最后我们留一道思考题吧。请你用下面的例子来模拟下决策树的流程，假设好苹果的数据如下，请用 ID3 算法来给出好苹果的决策树。\n",
    "\n",
    "<img src=\"./images/17-16.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-documentary",
   "metadata": {},
   "source": [
    "1. 計算純度\n",
    "\n",
    "|集合|好蘋果|壞蘋果|好蘋果機率|壞蘋果機率|\n",
    "|----|----|----|----|----|\n",
    "|紅|1|1|1/2|1/2|\n",
    "|大|1|1|1/2|1/2|\n",
    "\n",
    "帶入函數：兩個訊息熵都是1.0\n",
    "```python\n",
    "from math import log\n",
    "\n",
    "apple_red = [1/2, 1/2]\n",
    "\n",
    "def entropy(lst):\n",
    "    res = 0\n",
    "    for i in lst:\n",
    "        res += log(i,2) * -i\n",
    "    return res\n",
    "\n",
    "entropy(apple_red)\n",
    "```\n",
    "\n",
    "2. 訊息增益計算\n",
    "* Gain(D, 紅)，紅作為屬性劃分，會有兩個葉子節點，分別『紅與不紅』，用『＋，－』代表好與壞\n",
    "#### 根節點訊息熵\n",
    "* 四條資料中，2個好蘋果，2個壞蘋果為1.0。\n",
    "Ent(D) = -(1/2*log(1/2,2) + 1/2*log(1/2,2))\n",
    "\n",
    "### 紅、不紅屬性\n",
    "#### 1 以紅色為根節點的兩個葉子節點訊息熵\n",
    "|屬性劃分|好壞蘋果的機率|算出Ent(D)|Ent(D)|\n",
    "|:----|:----:|:----|----|\n",
    "|D1(紅=紅) = {1+, 2+}|[2/2, 0/2]|Ent(D1) = -(2/2*log(2/2,2) - 0/2*log(0/2,2))|0|\n",
    "|D2(紅=不紅) = {3-, 4-}|[0/2, 2/2]|Ent(D2) = -(0/2*log(0/2,2) - 2/2*log(2/2,2))|0|\n",
    "\n",
    "#### 2 歸化兩個葉子節點訊息熵>歸化訊息熵\n",
    "* D總共4筆資料，D1為2筆，D2為2筆，D1筆數佔全資料率為1/2, D2為1/2。帶入信息熵函數，D1與D2都是0。歸化訊息熵等於 1/2*0 + 1/2*0 = 0\n",
    "Gain(D, 紅) = 根節點訊息熵 - 歸化訊息熵 = 1 - 0 = 1\n",
    "\n",
    "\n",
    "### 大、小屬性\n",
    "#### 1 以大為跟節點的兩個葉子節點訊息熵\n",
    "|屬性劃分|好壞蘋果的機率|算出Ent(D)|Ent(D)|\n",
    "|:----|:----:|:----|----|\n",
    "|D1(大=大) = {1+, 3-}|[1/2, 1/2]|Ent(D1) = -(1/2*log(1/2,2) - 1/2*log(1/2,2))|1.0|\n",
    "|D2(大=小) = {2+, 4-}|[1/2, 1/2]|Ent(D2) = -(1/2*log(1/2,2) - 1/2*log(1/2,2))|1.0|\n",
    "\n",
    "#### 2 歸化兩個葉子節點訊息熵>歸化訊息熵\n",
    "* D總共4筆資料，D1為2筆，D2為2筆，D1筆數佔全資料率為1/2, D2為1/2。帶入信息熵函數，D1與D2都是0。**歸化訊息熵**等於 1/2*1 + 1/2*1 = 1\n",
    "Gain(D, 大) = 根節點訊息熵 - 歸化訊息熵 = 1 - 1 = 0\n",
    "\n",
    "### 選擇『紅』作為父節點\n",
    "由於Gain(D, 紅)為1 > Gain(D, 大)為0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "encouraging-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "apple_red = [1/2, 1/2]\n",
    "lst = [1/6, 5/6]\n",
    "\n",
    "# 訊息熵\n",
    "def entropy(lst):\n",
    "    res = 0\n",
    "    for i in lst:\n",
    "        if i != 0:\n",
    "            res += log(i,2) * -i\n",
    "    return res\n",
    "\n",
    "entropy(apple_red)\n",
    "\n",
    "# 訊息增益\n",
    "def gain_ent(ent:float, lst:list):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "looking-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "D1 = [1, 0]\n",
    "D2 = [0, 1]\n",
    "\n",
    "ent_d1 = entropy(D1)\n",
    "ent_d2 = entropy(D2)\n",
    "print(ent_d1, ent_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "permanent-pepper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = [1/2, 1/2]\n",
    "entropy(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
