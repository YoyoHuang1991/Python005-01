{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-heritage",
   "metadata": {},
   "source": [
    "#### 然后我们思考下，有了用户，用户画像都可以统计到哪些标签。我们按照“用户消费行为分析”的准则来进行设计。\n",
    "1. 用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。\n",
    "2. 消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。\n",
    "3. 行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。\n",
    "4. 内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。\n",
    "\n",
    "当你有了“用户消费行为分析”的标签之后，你就可以更好地理解业务了。\n",
    "比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。\n",
    "有了这些数据，我们就可以预测用户的行为。比如一个用户购买了“月子餐”后，更有可能购买婴儿水，同样婴儿相关的产品比如婴儿湿巾等的购买概率也会增大。\n",
    "#### 具体在业务层上，我们都可以基于标签产生哪些业务价值呢？\n",
    "1. 在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。\n",
    "2. 在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。\n",
    "3. 在留客上，预测用户是否可能会从平台上流失。\n",
    "4. 在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。\n",
    "\n",
    "<strong>但我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。对数据的标签化能让我们快速理解一个用户，一个商品，乃至一个视频内容的特征，从而方便我们去理解和使用数据。</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-bullet",
   "metadata": {},
   "source": [
    "08 | 数据采集：如何自动化采集数据？\n",
    "====\n",
    "这四类数据源包括了：开放数据源、爬虫抓取、传感器和日志采集\n",
    "### 數據源\n",
    "1. 開放數據源: 政府、企業、高校\n",
    "2. 爬蟲抓取: 網頁、app\n",
    "3. 傳感器: 前端採集、後端腳本\n",
    "4. 日誌採集: 圖像、測速、熱敏\n",
    "\n",
    "### 1. 單維度的數據源：\n",
    "|單位|數據源|網址|\n",
    "|----|----|----|\n",
    "|美國人口調查局|提供人口信息，地區分佈和教育情況等美國公民相關的數據|http://www.census.gov/data.html|\n",
    "|歐盟|歐盟開放數據平台，提供歐盟個機構的大量數據|http://open-data.europa.eu/en/data/|\n",
    "|Facebook|官方提供的API，用於查詢該網站用戶公開的海量信息|https://developers.facebook.com/docs/graph-api|\n",
    "|Amazon||http://aws.amazon.com/datasets|\n",
    "|Google||https://www.google.com/finance|\n",
    "|北京大學||http://opendata.pku.edu.cn/|\n",
    "|ImageNet|目前世界上圖像識別最大的數據庫，包括近1500萬張圖像|http://www.image-net.org/|\n",
    "\n",
    "### 2. Python爬蟲三個過程\n",
    "1. Request (http庫)爬取\n",
    "2. XPath(XML Path)解析\n",
    "3. Pandas保存，寫到xls或MySQL\n",
    "\n",
    "### 3. 其他無頭模式: Selenium, PhantomJS, Puppeteer\n",
    "* 不編程的抓取工具\n",
    "1. 火車採集器 http://www.locoy.com/\n",
    "2. 八抓魚(雲採集，自動切換IP) https://www.bazhuayu.com/\n",
    "3. 集搜客 http://www.gooseeker.com/\n",
    "\n",
    "### 4. 日誌採集工具\n",
    "1. 通過Web服務器採集，例如httpd，Nginx，Tomcat都自帶日誌記錄功能。同時很多互聯網企業都有自己的海量數據採集工具，如Hadoop的Chukwa，Cloudera的Flume，Facebook的Scribe等，這些工具均採用分佈式架構，能夠滿足數百萬兆字節的數據採集和傳輸需求。\n",
    "2. 自定義採集用戶行為，例如使用JavaScript代碼監聽用戶的行為，AJAX異步請求後台記錄日誌等。\n",
    "\n",
    "### 5. 埋點\n",
    "1. 埋點就是在你需要統計數據的地方植入統計代碼，當然植入代碼可以自己寫，也可以使用第三方統計工具。\n",
    "2. 我之前講到“不重複造輪子”的原則，一般來說需要自己寫的代碼，一般是主營核心業務，對於埋點和類別的監測性的工具，市場上已經比較成熟，這裡推薦你使用第三方的工具，諸如友盟，Google Analysis，Talkingdata等。他們都是採用前端埋點的方式，然後在第三方工具裡就可以看到用戶的行為數據。但如果我們想要看到更深層的用戶操作行為，就需要進行自定義埋點。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-motorcycle",
   "metadata": {},
   "source": [
    "09數據採集: 如何用八抓魚採集微博上的\"D&G\"評論\n",
    "====\n",
    "### 基本步骤\n",
    "就是最常用的步骤，每次采集都会用到，一共分为 4 步\n",
    "1. 打开网页\n",
    "2. 点击元素\n",
    "3. 循环翻页\n",
    "4. 提取数据\n",
    "\n",
    "### 使用的建议：\n",
    "1. 尽量使用用户操作视角进行模拟的方式进行操作，而不是在“流程视图”中手动创建相应的步骤。因为八爪鱼最大的特点就是所见即所得，所以一切就按照用户使用的流程进行操作即可。\n",
    "2. 使用“流程视图”方便管理和调整。右侧有“流程视图”的按钮，点击之后进入到流程视图，会把你之前的操作以流程图的方式展示出来。我会在文章下面详细介绍一下。\n",
    "\n",
    "### 采集微博上的“Dolce&Gabbana”评论\n",
    "1. 输入网页\n",
    "2. 输入关键词\n",
    "3. 点击搜索\n",
    "4. 设置翻页\n",
    "5. 提取数据\n",
    "6. 启动采集\n",
    "#### 两个重要的工具一定要用好：流程视图和 XPath。\n",
    "\n",
    "### 什么要讲一个八爪鱼这样的第三方工具呢？\n",
    "我们的工作流程通常很长，所以更应该专注工作的核心，比如说数据分析这块，所有的辅助都可以采用第三方工具来做。如果老板让你统计微博上的评论，实际上老板最想知道的不是采集的过程，而是整体的概况，比如说影响了多少人，评论如何，是否有 KOL 关注等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "illegal-beijing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-53b6c31033bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Requests 访问页面\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.douban.com'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 如果我们想要使用 Post 进行表单传递，代码就可以这样写：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://xxx.com'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "# Requests 访问页面\n",
    "r = requests.get('http://www.douban.com')\n",
    "\n",
    "# 如果我们想要使用 Post 进行表单传递，代码就可以这样写：\n",
    "r = requests.post('http://xxx.com', data = {'key':'value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-female",
   "metadata": {},
   "source": [
    "#### XPath定位\n",
    "XPath 是 XML 的路径语言，实际上是通过元素和属性进行导航，帮我们定位位置。它有几种常用的路径表达方式。\n",
    "\n",
    "|表達式|含義|\n",
    "|:----:|:----|\n",
    "|node|選node節點的所有子節點|\n",
    "|/|從根節點選取|\n",
    "|//|選取所有的當前節點，不考慮他們的位置|\n",
    "|.|當前節點|\n",
    "|..|父節點|\n",
    "|@|屬性選擇|\n",
    "|||或，兩個節點的合計|\n",
    "|text()|當前路徑下的文本內容|\n",
    "\n",
    "1. xpath（'node'）選擇節點的所有子節點； \n",
    "2. xpath（'/ div'）從根節點上選擇div節點； \n",
    "3. xpath（'// div'）選擇所有的div節點； \n",
    "4. xpath（'./ div '）選擇當前例程下的div節點； \n",
    "5. xpath（'...'）返回上一個例程； \n",
    "6. xpath（'// @ id'）選擇所有的id屬性； \n",
    "7. xpath（'// book [@id]'）選擇所有擁有稱為id的屬性的book元素； \n",
    "8. xpath（'// book [@ id =“ abc”]'）選擇所有書籍元素，且這些圖書元素擁有id =“ abc”的屬性； \n",
    "9. xpath（'// book / title | // book / price'）選取book元素的所有title和price元素。\n",
    "\n",
    "使用 XPath 定位，你会用到 Python 的一个解析库 lxml。这个库的解析效率非常高，使用起来也很简便，只需要调用 HTML 解析命令即可，然后再对 HTML 进行 XPath 函数的调用。比如我们想要定位到 HTML 中的所有列表项目，可以采用下面这段代码。\n",
    "```python\n",
    "from lxml import etree\n",
    "html = etree.HTML(html)\n",
    "result = html.xpath('//li')\n",
    "```\n",
    "\n",
    "### JSON 对象\n",
    "JSON 是一种轻量级的交互方式，在 Python 中有 JSON 库，可以让我们将 Python 对象和 JSON 对象进行转换。为什么要转换呢？原因也很简单。将 JSON 对象转换成为 Python 对象，我们对数据进行解析就更方便了。\n",
    "\n",
    "|方法|含義|\n",
    "|:----|:----|\n",
    "|json.dumps()|將python對象轉換成JSON對象|\n",
    "|json.loads()|將json對象傳換成python對象|\n",
    "\n",
    "这是一段将 JSON 格式转换成 Python 对象的代码，你可以自己运行下这个程序的结果。\n",
    "```python\n",
    "import json\n",
    "jsonData = '{\"a\":1,\"b\":2,\"c\":3,\"d\":4,\"e\":5}';\n",
    "input = json.loads(jsonData)\n",
    "print input\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-dominant",
   "metadata": {},
   "source": [
    "### 如何使用 JSON 数据自动下载王祖贤的海报\n",
    "#### 如何查看呢，我们再来重新看下这个网址本身。\n",
    "* https://www.douban.com/j/search_photo?q=王祖贤&limit=20&start=0\n",
    "\n",
    "- 你会发现，网址中有三个参数：q、limit 和 start。start 实际上是请求的起始 ID，这里我们注意到它对图片的顺序标识是从 0 开始计算的。所以如果你想要从第 21 个图片进行下载，你可以将 start 设置为 20。\n",
    "- 王祖贤的图片一共有 22471 张，你可以写个 for 循环来跑完所有的请求，具体的代码如下：\n",
    "\n",
    "```python\n",
    "# coding:utf-8\n",
    "import requests\n",
    "import json\n",
    "query = '王祖贤'\n",
    "''' 下载图片 '''\n",
    "def download(src, id):\n",
    "  dir = './' + str(id) + '.jpg'\n",
    "  try:\n",
    "    pic = requests.get(src, timeout=10)\n",
    "    fp = open(dir, 'wb')\n",
    "    fp.write(pic.content)\n",
    "    fp.close()\n",
    "  except requests.exceptions.ConnectionError:\n",
    "    print('图片无法下载')\n",
    "            \n",
    "''' for 循环 请求全部的 url '''\n",
    "for i in range(0, 22471, 20):\n",
    "  url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)\n",
    "  html = requests.get(url).text    # 得到返回结果\n",
    "  response = json.loads(html,encoding='utf-8') # 将 JSON 格式转换成 Python 对象\n",
    "  for image in response['images']:\n",
    "    print(image['src']) # 查看当前下载的图片网址\n",
    "    download(image['src'], image['id']) # 下载一张图片\n",
    "```\n",
    "\n",
    "### 如何使用 XPath 自动下载王祖贤的电影海报封面\n",
    "我们想要从豆瓣电影上下载王祖贤的电影封面，需要先梳理下人工的操作流程：\n",
    "1. 打开网页 movie.douban.com；\n",
    "2. 输入关键词“王祖贤”；\n",
    "3. 下载图片页中的所有电影封面。\n",
    "这里你需要用 XPath 定位图片的网址，以及电影的名称。采用浏览器的 XPath Helper 插件，使用 Ctrl+Shift+X 快捷键。\n",
    "你可以得到电影海报的 XPath（假设为变量 src_xpath）\n",
    "```\n",
    "//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src\n",
    "```\n",
    "以及电影名称的 XPath（假设为变量 title_xpath）：\n",
    "```\n",
    "//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']\n",
    "```\n",
    "\n",
    "但有时候当我们直接用 Requests 获取 HTML 的时候，发现想要的 XPath 并不存在。这是因为 HTML 还没有加载完，因此你需要一个工具，来进行网页加载的模拟，直到完成加载后再给你完整的 HTML。在 Python 中，这个工具就是 Selenium 库，使用方法如下：\n",
    "```python\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(request_url)\n",
    "```\n",
    "\n",
    "当你获取到完整的 HTML 时，就可以对 HTML 中的 XPath 进行提取，在这里我们需要找到图片地址 srcs 和电影名称 titles。这里通过 XPath 语法匹配到了多个元素，因为是多个元素，所以我们需要用 for 循环来对每个元素进行提取。\n",
    "```python\n",
    "srcs = html.xpath(src_xpath)\n",
    "titles = html.xpath(title_path)\n",
    "for src, title in zip(srcs, titles):\n",
    "  download(src, title.text)\n",
    "```\n",
    "\n",
    "<img src=\"./images/10-01.jpg\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-china",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
