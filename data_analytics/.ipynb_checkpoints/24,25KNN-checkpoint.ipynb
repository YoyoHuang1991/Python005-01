{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooked-hartford",
   "metadata": {},
   "source": [
    "24KNN K-Nearest Neighbor(上)如何根據打鬥和接吻次數來劃分電影類型\n",
    "====\n",
    "\n",
    "## 工作原理\n",
    "“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。整个计算过程分为三步：\n",
    "1. 计算待分类物体与其他物体之间的距离；\n",
    "2. 统计距离最近的 K 个邻居；\n",
    "3. 对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。\n",
    "\n",
    "## K值如何選擇\n",
    "* 如果 K 值*比较小*，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果**邻居点是个噪声点**，那么未分类物体的分类也会产生**误差**，这样 KNN 分类就会产生过拟合。\n",
    "* 如果 K 值*比较大*，相当于**距离过远**的点也会对未知物体的分类产生影响，虽然这种情况的好处是**鲁棒性强**，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。\n",
    "* 所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用**交叉验证的方式选取 K 值**。\n",
    "* **交叉验证**的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 **K 值选取在较小的范围内**，同时在**验证集上准确率最高的那一个最终确定作为 K 值**。\n",
    "\n",
    "## 距離如何計算\n",
    "两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。关于距离的计算方式有下面五种方式：\n",
    "1. 欧氏距离；\n",
    "2. 曼哈顿距离；\n",
    "3. 闵可夫斯基距离；\n",
    "4. 切比雪夫距离；\n",
    "5. 余弦距离。\n",
    "\n",
    "其中**前三种距离是 KNN 中最常用的距离**，我给你分别讲解下。\n",
    "\n",
    "### 1. 欧氏距离是我们最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：\n",
    "\n",
    "<img src=\"./images/24-01.png\">\n",
    "同理，我们也可以求得两点在 n 维空间中的距离：\n",
    "<img src=\"./images/24-02.png\">\n",
    "\n",
    "### 2. 曼哈顿距离在几何空间中用的比较多。\n",
    "以下图为例，绿色的直线代表两点之间的欧式距离，而红色和黄色的线为两点的曼哈顿距离。所以曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：\n",
    "<img src=\"./images/24-03.png\">\n",
    "<img src=\"./images/24-04.jpg\">\n",
    "\n",
    "### 3. 闵可夫斯基距离不是一个距离，而是一组距离的定义。\n",
    "对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：\n",
    "<img src=\"./images/24-05.png\">\n",
    "* 其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。\n",
    "\n",
    "### 4. 切比雪夫距离\n",
    "二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。\n",
    "### 5. 余弦距离\n",
    "实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。\n",
    "\n",
    "## KD樹\n",
    "KNN 的计算过程是大量计算**样本点之间的距离**。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，**每个节点都是 k 维数值点的二叉树**。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。\n",
    "\n",
    "在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。\n",
    "\n",
    "## 用KNN做回歸\n",
    "KNN 不仅可以做分类，还可以做回归。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。\n",
    "\n",
    "如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。\n",
    "\n",
    "对于一个新电影 X，我们要预测它的某个属性值，比如打斗次数，具体特征属性和数值如下所示。此时，我们会先计算待测点（新电影 X）到已知点的距离，选择距离最近的 K 个点。假设 K=3，此时最近的 3 个点（电影）分别是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数就是这 3 个点的该属性值的平均值，即 (100+95+105)/3=100 次。\n",
    "<img src=\"./images/24-06.png\">\n",
    "\n",
    "## 總結\n",
    "今天我给你讲了 KNN 的原理，以及 KNN 中的几个关键因素。比如针对 K 值的选择，我们一般采用交叉验证的方式得出。针对样本点之间的距离的定义，常用的有 5 种表达方式，你也可以自己来定义两个样本之间的距离公式。不同的定义，适用的场景不同。比如在搜索关键词推荐中，余弦距离是更为常用的。\n",
    "\n",
    "另外你也可以用 KNN 进行回归，通过 K 个邻居对新的点的属性进行值的预测。\n",
    "\n",
    "KNN 的理论简单直接，针对 KNN 中的搜索也有相应的 KD 树这个数据结构。KNN 的理论成熟，可以应用到线性和非线性的分类问题中，也可以用于回归分析。\n",
    "\n",
    "不过 KNN 需要计算测试点与样本点之间的距离，当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间。另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。\n",
    "\n",
    "当然在实际工作中，我们需要考虑到各种可能存在的情况，比如针对某类**样本少**的情况，可以增加**该类别的权重**。\n",
    "\n",
    "同样 KNN 也可以用于推荐算法，虽然现在很多推荐系统的算法会使用 TD-IDF、协同过滤、Apriori 算法，不过针对数据量不大的情况下，采用 KNN 作为推荐算法也是可行的。\n",
    "<img src=\"./images/24-07.png\">\n",
    "\n",
    "\n",
    "思考題\n",
    "====\n",
    "KNN 的算法原理和工作流程是怎么样的？KNN 中的 K 值又是如何选择的？\n",
    "\n",
    "查看breat_linearsvm.py\n",
    "<img src=\"./images/24-08.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-avenue",
   "metadata": {},
   "source": [
    "25丨KNN（下）：如何对手写数字进行识别？\n",
    "====\n",
    "如何在 sklearn 中使用 KNN 算法，然后通过 sklearn 中自带的手写数字数据集来进行实战。\n",
    "\n",
    "## 如何在 sklearn 中使用 KNN\n",
    "在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。如果是做分类，你需要引用：\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "如果是做回归，你需要引用：\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "```\n",
    "从名字上你也能看出来 Classifier 对应的是分类，Regressor 对应的是回归。一般来说如果一个算法有 Classifier 类，都能找到相应的 Regressor 类。比如在决策树分类中，你可以使用 DecisionTreeClassifier，也可以使用决策树来做回归 DecisionTreeRegressor。\n",
    "\n",
    "好了，我们看下如何在 sklearn 中创建 KNN 分类器。\n",
    "\n",
    "这里，我们使用构造函数 **KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)**，这里有几个比较主要的参数，我分别来讲解下：\n",
    "\n",
    "1. n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。\n",
    "2. weights：是用来确定邻居的权重，有三种方式：\n",
    "    * weights=uniform，代表所有邻居的权重相同；\n",
    "    * weights=distance，代表权重是距离的倒数，即与距离成反比；自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。\n",
    "3. algorithm：用来规定计算邻居的方法，它有四种方式：\n",
    "    * algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；\n",
    "    * algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；\n",
    "    * algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；\n",
    "    * algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。\n",
    "4. leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。\n",
    "\n",
    "创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。\n",
    "\n",
    "## 如何用 KNN 对手写数字进行识别分类\n",
    "**手写数字数据集**是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。\n",
    "\n",
    "今天我们用 sklearn **自带的手写数字数据集**做 KNN 分类，你可以把这个数据集理解成一个简版的 MNIST 数据集，它只包括了 1797 幅数字图像，每幅图像大小是 8*8 像素。\n",
    "<img src=\"./images/25-01.jpg\">\n",
    "\n",
    "整个训练过程基本上都会包括三个阶段：\n",
    "1. 数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；\n",
    "2. 准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8*8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；\n",
    "3. 分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。\n",
    "```python\n",
    "# 加载数据\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "# 数据探索\n",
    "print(data.shape)\n",
    "# 查看第一幅图像\n",
    "print(digits.images[0])\n",
    "# 第一幅图像代表的数字含义\n",
    "print(digits.target[0])\n",
    "# 将第一幅图像显示出来\n",
    "plt.gray()\n",
    "plt.imshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个 8\\*8 的像素矩阵，上面这幅图像是一个“0”，从训练集的分类标注中我们也可以看到分类标注为“0”。\n",
    "\n",
    "sklearn 自带的手写数字数据集一共包括了 1797 个样本，每幅图像都是 8\\*8 像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化，代码如下：\n",
    "\n",
    "```python\n",
    "# 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）\n",
    "train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)\n",
    "# 采用Z-Score规范化\n",
    "ss = preprocessing.StandardScaler()\n",
    "train_ss_x = ss.fit_transform(train_x)\n",
    "test_ss_x = ss.transform(test_x)\n",
    "```\n",
    "\n",
    "然后我们构造一个 KNN 分类器 knn，把训练集的数据传入构造好的 knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到 KNN 分类器准确率，代码如下：\n",
    "```python\n",
    "# 创建KNN分类器\n",
    "knn = KNeighborsClassifier() \n",
    "knn.fit(train_ss_x, train_y) \n",
    "predict_y = knn.predict(test_ss_x) \n",
    "print(\"KNN准确率: %.4lf\" % accuracy_score(test_y, predict_y))\n",
    "```\n",
    "好了，这样我们就构造好了一个 KNN 分类器。之前我们还讲过 SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：\n",
    "```python\n",
    "# 创建SVM分类器\n",
    "svm = SVC()\n",
    "svm.fit(train_ss_x, train_y)\n",
    "predict_y=svm.predict(test_ss_x)\n",
    "print('SVM准确率: %0.4lf' % accuracy_score(test_y, predict_y))\n",
    "# 采用Min-Max规范化\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "train_mm_x = mm.fit_transform(train_x)\n",
    "test_mm_x = mm.transform(test_x)\n",
    "# 创建Naive Bayes分类器\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_mm_x, train_y) \n",
    "predict_y = mnb.predict(test_mm_x) \n",
    "print(\"多项式朴素贝叶斯准确率: %.4lf\" % accuracy_score(test_y, predict_y))\n",
    "# 创建CART决策树分类器\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(train_mm_x, train_y) \n",
    "predict_y = dtc.predict(test_mm_x) \n",
    "print(\"CART决策树准确率: %.4lf\" % accuracy_score(test_y, predict_y))\n",
    "```\n",
    "这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为 Z-Score 会将数值规范化为一个标准的正态分布，即均值为 0，方差为 1，数值会包含负数。因此我们需要采用 Min-Max 规范化，将数据规范化到[0,1]范围内。\n",
    "\n",
    "好了，我们整理下这 4 个分类器的结果。\n",
    "<img src=\"./images/25-02.png\">\n",
    "\n",
    "你能看出来 KNN 的准确率还是不错的，和 SVM 不相上下。\n",
    "\n",
    "你可以自己跑一遍整个代码，在运行前还需要 import 相关的工具包（下面的这些工具包你都会用到，所以都需要引用）：\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "代码中，我使用了 train_test_split 做数据集的拆分，使用 matplotlib.pyplot 工具包显示图像，使用 accuracy_score 进行分类器准确率的计算，使用 preprocessing 中的 StandardScaler 和 MinMaxScaler 做数据的规范化。\n",
    "\n",
    "总结\n",
    "====\n",
    "今天我带你一起做了手写数字分类识别的实战，分别用 KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在这个过程中你应该对数据探索、数据可视化、数据规范化、模型训练和结果评估的使用过程有了一定的体会。在数据量不大的情况下，使用 sklearn 还是方便的。\n",
    "\n",
    "如果数据量很大，比如 MNIST 数据集中的 6 万个训练数据和 1 万个测试数据，那么采用深度学习 +GPU 运算的方式会更适合。因为深度学习的特点就是需要大量并行的重复计算，GPU 最擅长的就是做大量的并行计算。\n",
    "<img src=\"./images/25-03.png\">\n",
    "\n",
    "思考题\n",
    "====\n",
    "请你说说项目中 KNN 分类器的常用构造参数，功能函数都有哪些，以及你对 KNN 使用的理解？如果把 KNN 中的 K 值设置为 200，数据集还是 sklearn 中的手写数字数据集，再跑一遍程序，看看分类器的准确率是多少？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-christian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
