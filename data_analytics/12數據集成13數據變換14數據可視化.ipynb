{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "injured-deadline",
   "metadata": {},
   "source": [
    "ETL 工具有哪些？\n",
    "====\n",
    "介绍完了这两种架构，你肯定想要知道 ETL 工具都有哪些？\n",
    "#### 典型的 ETL 工具有:\n",
    "* 商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等\n",
    "* 开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等相对于传统的商业软件，Kettle 是一个易于使用的，低成本的解决方案。国内很多公司都在使用 Kettle 用来做数据集成。所以我重点给你讲解下 Kettle 工具的使用\n",
    "\n",
    "### Kettle \n",
    "在 2006 年并入了开源的商业智能公司 Pentaho, 正式命名为 Pentaho Data Integeration，简称“PDI”。因此 Kettle 现在是 Pentaho 的一个组件，下载地址：https://community.hitachivantara.com/docs/DOC-1009855\n",
    "\n",
    "Kettle 采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation 转换和 Job 作业。\n",
    "* Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。\n",
    "* Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。\n",
    "\n",
    "在 Transformation 中包括两个主要概念：Step 和 Hop。Step 的意思就是步骤，Hop 就是跳跃线的意思。\n",
    "* Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这 4 个步骤；\n",
    "* Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。\n",
    "\n",
    "<b>如何创建 Job（作业）：</b>\n",
    "完整的任务，实际上是将创建好的转换和作业串联起来。在这里 Job 包括两个概念：Job Entry、Hop。\n",
    "* Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。\n",
    "* Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。\n",
    "\n",
    "#### 案例 1：如何将文本文件的内容转化到 MySQL 数据库中\n",
    "这里我给你准备了文本文件，这个文件我上传到了 GitHub 上，你可以自行下载：http://t.cn/E4SzvOf\n",
    "数据描述如下：\n",
    "* Step1：创建转换，右键“转换→新建”；\n",
    "* Step2：在左侧“核心对象”栏目中选择“文本文件输入”控件，拖拽到右侧的工作区中；\n",
    "* Step3：从左侧选择“表输出”控件，拖拽到右侧工作区；\n",
    "* Step4：鼠标在“文本文件输入”控件上停留，在弹窗中选择图标，鼠标拖拽到“表输出”控件，将一条连线连接到两个控件上；这时我们已经将转换的流程设计好了，现在是要对输入和输出两个控件进行设置。\n",
    "* Step5：双击“文本文件输入”控件，导入已经准备好的文本文件；\n",
    "* Step6：双击“表输出”控件，这里你需要配置下 MySQL 数据库的连接，同时数据库中需要有一个数据表，字段的设置与文本文件的字段设置一致（这里我设置了一个 wucai 数据库，以及 score 数据表。字段包括了 name、create_time、Chinese、English、Math，与文本文件的字段一致）。\n",
    "* Step7：创建数据库字段的对应关系，这个需要双击“表输出”，找到数据库字段，进行字段映射的编辑；\n",
    "* Step8：点击左上角的执行图标，如下图：\n",
    "#### Kettle 的开源社区：http://www.ukettle.org 。\n",
    "\n",
    "### 阿里开源软件：DataX\n",
    "### Apache 开源软件:Sqoop\n",
    "https://sqoop.apache.org/\n",
    "\n",
    "Sqoop 是一款开源的工具，是由 Apache 基金会所开发的分布式系统基础架构。Sqoop 在 Hadoop 生态系统中是占据一席之地的，它主要用来在 Hadoop 和关系型数据库中传递数据。通过 Sqoop，我们可以方便地将数据从关系型数据库导入到 HDFS 中，或者将数据从 HDFS 导出到关系型数据库中。Hadoop 实现了一个分布式文件系统，即 HDFS。Hadoop 的框架最核心的设计就是 HDFS 和 MapReduce。HDFS 为海量的数据提供了存储，而 MapReduce 则为海量的数据提供了计算。\n",
    "\n",
    "<img src=\"./images/12-01.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-heart",
   "metadata": {},
   "source": [
    "數據變換\n",
    "====\n",
    "所以你从整个流程中可以看出，数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。我来介绍下这些常见的变换方法：\n",
    "1. 数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；\n",
    "2. 数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；\n",
    "3. 数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。\n",
    "4. 数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；\n",
    "5. 属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。\n",
    "\n",
    "数据规范化的几种方法\n",
    "1. Min-max 规范化Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。\n",
    "2. Z-Score 规范化假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义\n",
    "3. 小数定标规范化小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。\n",
    "\n",
    "### Python 的 SciKit-Learn 库使用\n",
    "SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。\n",
    "1. Min-max 规范化\n",
    "我们可以让原始数据投射到指定的空间[min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。默认情况下[min,max]是[0,1]，也就是把原始数据投放到[0,1]范围内。\n",
    "\n",
    "```python\n",
    "# coding:utf-8\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 初始化数据，每一行表示一个样本，每一列表示一个特征\n",
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "# 将数据进行[0,1]规范化\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "minmax_x = min_max_scaler.fit_transform(x)\n",
    "print minmax_x\n",
    "```\n",
    "\n",
    "2. Z-Score 规范化在 \n",
    "SciKit-Learn 库中使用 preprocessing.scale() 函数，可以直接将给定数据进行 Z-Score 规范化。\n",
    "这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。我们看到 Z-Score 规范化将数据集进行了规范化，数值都符合均值为 0，方差为 1 的正态分布。\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 初始化数据\n",
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "# 将数据进行Z-Score规范化\n",
    "scaled_x = preprocessing.scale(x)\n",
    "print scaled_x\n",
    "```\n",
    "\n",
    "3. 小数定标规范化\n",
    "我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。这里我们看下运行代码：\n",
    "\n",
    "```python\n",
    "# coding:utf-8\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 初始化数据\n",
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "# 小数定标规范化\n",
    "j = np.ceil(np.log10(np.max(abs(x))))\n",
    "scaled_x = x/(10**j)\n",
    "print scaled_x\n",
    "```\n",
    "\n",
    "### 数据挖掘中数据变换比算法选择更重要\n",
    "在考试成绩中，我们都需要让数据满足一定的规律，达到规范性的要求，便于进行挖掘。这就是数据变换的作用。\n",
    "如果不进行变换的话，要不就是维数过多，增加了计算的成本，要不就是数据过于集中，很难找到数据之间的特征。\n",
    "在数据变换中，重点是如何将数值进行规范化，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 Z-Score 规范化可以直接将数据转化为正态分布的情况，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。\n",
    "\n",
    "### 思考题\n",
    "假设属性 income 的最小值和最大值分别是 5000 元和 58000 元。利用 Min-Max 规范化的方法将属性的值映射到 0 至 1 的范围内，那么属性 income 的 16000 元将被转化为多少？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pressing-polyester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.          66.66666667]\n",
      " [100.         100.         100.        ]\n",
      " [  0.         100.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 初始化数据，每一行表示一个样本，每一列表示一个特征\n",
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "# 将数据进行[0,1]规范化\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,100))\n",
    "minmax_x = min_max_scaler.fit_transform(x)\n",
    "print(minmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removed-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -3.,  1.],\n",
       "       [ 3.,  1.,  2.],\n",
       "       [ 0.,  1., -1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cooked-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  MinMaxScaler(feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  clip: bool, default=False\n",
      " |      Set to True to clip transformed values of held-out data to\n",
      " |      provided `feature range`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  minmax_scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True, clip=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessing.MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "persistent-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.70710678 -1.41421356  0.26726124]\n",
      " [ 1.41421356  0.70710678  1.06904497]\n",
      " [-0.70710678  0.70710678 -1.33630621]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 初始化数据\n",
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "# 将数据进行Z-Score规范化\n",
    "scaled_x = preprocessing.scale(x)\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occupied-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scale in module sklearn.preprocessing._data:\n",
      "\n",
      "scale(X, *, axis=0, with_mean=True, with_std=True, copy=True)\n",
      "    Standardize a dataset along any axis.\n",
      "    \n",
      "    Center to the mean and component wise scale to unit variance.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "        The data to center and scale.\n",
      "    \n",
      "    axis : int, default=0\n",
      "        axis used to compute the means and standard deviations along. If 0,\n",
      "        independently standardize each feature, otherwise (if 1) standardize\n",
      "        each sample.\n",
      "    \n",
      "    with_mean : bool, default=True\n",
      "        If True, center the data before scaling.\n",
      "    \n",
      "    with_std : bool, default=True\n",
      "        If True, scale the data to unit variance (or equivalently,\n",
      "        unit standard deviation).\n",
      "    \n",
      "    copy : bool, default=True\n",
      "        set to False to perform inplace row normalization and avoid a\n",
      "        copy (if the input is already a numpy array or a scipy.sparse\n",
      "        CSC matrix and if axis is 1).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "        The transformed data.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This implementation will refuse to center scipy.sparse matrices\n",
      "    since it would make them non-sparse and would potentially crash the\n",
      "    program with memory exhaustion problems.\n",
      "    \n",
      "    Instead the caller is expected to either set explicitly\n",
      "    `with_mean=False` (in that case, only variance scaling will be\n",
      "    performed on the features of the CSC matrix) or to call `X.toarray()`\n",
      "    if he/she expects the materialized dense array to fit in memory.\n",
      "    \n",
      "    To avoid memory copy the caller should pass a CSC matrix.\n",
      "    \n",
      "    NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "    and maintained during the data transformation.\n",
      "    \n",
      "    We use a biased estimator for the standard deviation, equivalent to\n",
      "    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "    affect model performance.\n",
      "    \n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    .. warning:: Risk of data leak\n",
      "    \n",
      "        Do not use :func:`~sklearn.preprocessing.scale` unless you know\n",
      "        what you are doing. A common mistake is to apply it to the entire data\n",
      "        *before* splitting into training and test sets. This will bias the\n",
      "        model evaluation because information would have leaked from the test\n",
      "        set to the training set.\n",
      "        In general, we recommend using\n",
      "        :class:`~sklearn.preprocessing.StandardScaler` within a\n",
      "        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n",
      "        leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    StandardScaler : Performs scaling to unit variance using the Transformer\n",
      "        API (e.g. as part of a preprocessing\n",
      "        :class:`~sklearn.pipeline.Pipeline`).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(preprocessing.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accredited-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  -0.3  0.1]\n",
      " [ 0.3  0.1  0.2]\n",
      " [ 0.   0.1 -0.1]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "# 初始化数据\n",
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "# 小数定标规范化\n",
    "j = np.ceil(np.log10(np.max(abs(x))))\n",
    "scaled_x = x/(10**j)\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "statistical-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[ 0., -3.,  1.],\n",
    "              [ 3.,  1.,  2.],\n",
    "              [ 0.,  1., -1.]])\n",
    "tmp = abs(x) #將所有值取絕對值\n",
    "tmp = np.max(abs(x))\n",
    "j = np.ceil(np.log10(np.max(abs(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "another-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x = x/(10**j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "subject-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. , -0.3,  0.1],\n",
       "       [ 0.3,  0.1,  0.2],\n",
       "       [ 0. ,  0.1, -0.1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-captain",
   "metadata": {},
   "source": [
    "14丨数据可视化：掌握数据领域的万金油技能\n",
    "====\n",
    "\n",
    "### 我们常用的可视化视图超过 20 种，分别包括：\n",
    "文本表、热力图、地图、符号地图、饼图、水平条、堆叠条、并排条、树状图、圆视图、并排圆、线、双线、面积图、双组合、散点图、直方图、盒须图、甘特图、靶心图、气泡图等。\n",
    "\n",
    "你不仅要掌握这些视图的使用，更要了解使用它们背后的目的是什么，这里我整理了下，可以分为以下的 9 种情况：\n",
    "\n",
    "|分佈</br> distribution|時間相關</br> change over time|局部/整體</br> part to whole|\n",
    "|:----|:----|:----|\n",
    "|偏差deviation|相關性correlation|排名ranking|\n",
    "|量級magnitude|地圖spatial|流動flow|\n",
    "\n",
    "#### 在设计之前，你需要思考的是，你的用户是谁，想给他们呈现什么，需要突出数据怎样的特点，以及采用哪种视图来进行呈现。\n",
    "1. 比如说，你想呈现某个变量的分布情况，就可以通过直方图的形式来呈现。\n",
    "2. 如果你想要看两个变量之间的相关性及分布情况，可以采用散点图的形式呈现。\n",
    "3. 散点图既可以表明两个变量之间的关系，也可以体现它们的分布情况。同样，如果我想看变量的分布情况，既可以采用散点图的形式，也可以采用直方图的形式\n",
    "\n",
    "#### 商业智能分析\n",
    "首先在商业智能分析软件中，最著名的当属 Tableau 和 PowerBI 了，另外中国帆软出品的 FineBI 也受到国内很多企业的青睐。\n",
    "\n",
    "#### 可视化大屏类\n",
    "* DataV\n",
    "* FineReport\n",
    "\n",
    "#### 前端可视化组件\n",
    "如果你想要成为一名前端数据可视化工程师的话，至少熟练掌握一种前端可视化组件是必不可少的，不少公司招聘“高级前端工程师”的时候，都要求熟悉几个开源数据可视化组件。\n",
    "* 可视化组件都是基于 Web 渲染的技术的。所以你需要了解一下几个典型的 Web 渲染技术：Canvas、SVG 和 WebGL。简单来说，Canvas 和 SVG 是 HTML5 中主要的 2D 图形技术，WebGL 是 3D 框架。\n",
    "\n",
    "1. Canvas 适用于位图，也就是给了你一张白板，需要你自己来画点。Canvas 技术可以绘制比较复杂的动画。不过它是 HTML5 自带的，所以低版本浏览器不支持 Canvas。ECharts 这个可视化组件就是基于 Canvas 实现的。\n",
    "2. SVG 的中文是可缩放矢量图形，它是使用 XML 格式来定义图形的。相当于用点和线来描绘了图形，相比于位图来说文件比较小，而且任意缩放都不会失真。SVG 经常用于图标和图表上。它最大的特点就是支持大部分浏览器，动态交互性实现起来也很方便，比如在 SVG 中插入动画元素等。\n",
    "3. WebGL 是一种 3D 绘图协议，能在网页浏览器中呈现 3D 画面技术，并且可以和用户进行交互。你在网页上看到的很多酷炫的 3D 效果，基本上都是用 WebGL 来渲染的。下面介绍的 Three.js 就是基于 WebGL 框架的。\n",
    "\n",
    "#### 在了解这些 Web 渲染协议之后，我再来带你看下这些常用的可视化组件： Echarts、D3、Three.js 和 AntV。\n",
    "1. ECharts 是基于 H5 canvas 的 Javascript 图表库，是百度的开源项目，一直都有更新，使用的人也比较多。它作为一个组件，可以和 DataV、Python 进行组合使用。</br>\n",
    "* 你可以在 DataV 企业版中接入 ECharts 图表组件。也可以使用 Python 的 Web 框架（比如 Django、Flask）+ECharts 的解决方案。这样可以让你的项目更加灵活地使用到 ECharts 的图表库，不论你是用 Python 语言，还是用 DataV 的工具，都可以享受到 ECharts 丰富的图表库样式。\n",
    "\n",
    "2. D3 的全称是 Data-Driven Documents，简单来说，是一个 JavaScript 的函数库，因为文件的后缀名通常为“.js”，所以 D3 也常使用 D3.js 来称呼。\n",
    "* 它提供了各种简单易用的函数，大大简化了 JavaScript 操作数据的难度。你只需要输入几个简单的数据，就能够转换为各种绚丽的图形。由于它本质上是 JavaScript，所以用 JavaScript 也是可以实现所有功能的。\n",
    "\n",
    "3. Three.js，顾名思义，就是 Three+JS 的意思。“Three”表示 3D 的意思，“Three.js”就是使用 JavaScript 来实现 3D 效果。Three.js 是一款 WebGL 框架，封装了大量 WebGL 接口，因为直接用 WebGL API 写 3D 程序太麻烦了。\n",
    "4. AntV 是蚂蚁金服出品的一套数据可视化组件，包括了 G2、G6、F2 和 L7 一共 4 个组件。其中 G2 应该是最知名的，它的意思是 The grammar Of Graphics，也就是一套图形语法。它集成了大量的统计工具，而且可以让用户通过简单的语法搭建出多种图表。G6 是一套流程图和关系分析的图表库。F2 适用于移动端的可视化方案。L7 提供了地理空间的数据可视化框架\n",
    "\n",
    "#### 编程语言\n",
    "1. 在 Python 里包括了众多可视化库，比如 Matplotlib、Seaborn、Bokeh、Plotly、Pyecharts、Mapbox 和 Geoplotlib。其中使用频率最高，最需要掌握的就是 Matplotlib 和 Seaborn。Matplotlib 是 Python 的可视化基础库，作图风格和 MATLAB 类似，所以称为 Matplotlib。一般学习 Python 数据可视化，都会从 Matplotlib 入手，然后再学习其他的 Python 可视化库。\n",
    "<img src=\"./images/14-01.jpg\">\n",
    "\n",
    "2. Seaborn 是一个基于 Matplotlib 的高级可视化效果库，针对 Matplotlib 做了更高级的封装，让作图变得更加容易。你可以用短小的代码绘制更多维度数据的可视化效果图，比如下面这个例子：\n",
    "<img src=\"./images/14-02.jpg\">\n",
    "\n",
    "3. 在 R 中也有很多可视化库可供选择。其中包括了 R 自带的绘图包 Graphics 以及工具包 ggplot2、ggmap、timevis 和 plotly 等。其中 ggplot2 是 R 语言中重要的绘图包，这个工具包将数据与绘图操作进行了分离，所以使用起来清晰明了，画出的图也漂亮。其实在 Python 里后来也引入了 ggplot 库，这样在 Python 中也可以很方便地使用到 ggplot，而且和 R 语言中的 ggplot2 代码差别不大，稍作修改，就能直接在 Python 中运行了。\n",
    "\n",
    "### 如何开始数据\n",
    "可视化的学习其实很多企业都有在用商业分析软件，Tableau 算是使用率很高的。如果你想做相关的数据分析研究，掌握一门语言尤其是 Python 还是很有必要的。如果你想要全面的学习数据可视化，你可以有以下的 3 个路径：\n",
    "1. 重点推荐 TableauTableau 在可视化灵活分析上功能强大，主要目标用户更多是较专业的数据分析师。同时在工作场景中使用率高，因此掌握 Tableau 对于晋升和求职都很有帮助。不过 Tableau 是个商业软件，收费不低。而且上手起来有一些门槛，需要一定数据基础。\n",
    "2.  使用微图、DataV前面我给你讲过八爪鱼的使用，微图和八爪鱼是一家公司的产品，使用起来非常方便，而且免费。当你用八爪鱼采集数据之后，就直接可以用微图进行数据可视化。DataV 是阿里推出的数字大屏技术，不过它是收费的产品。它最大的好处，就是可以分享链接，让别人可以在线浏览，不需要像 Tableau 一样安装客户端才能看到数据可视化的结果。另外 DataV 有一些模板，你直接可以使用。你可以先使用微图和 DataV 作为你的数据可视化体验工具，因为成本低，上手起来快。这样你对数据可视化可以有个直观的了解。如果你想从事数据可视化这份工作，你可以花更多的精力去研究和学习 Tableau。\n",
    "3. Python 可视化Python 是数据分析的首选语言，如果你不进行编程，可以使用我在上文中提到的数据可视化的工具。如果你的目标是个数据挖掘工程师，或者算法工程师，那么最重要的就是要了解，并且熟练掌握 Python 的数据可视化。\n",
    "\n",
    "<img src='./images/14-03.jpg'>\n",
    "\n",
    "两道思考题\n",
    "====\n",
    "你平时的工作是怎样的，是否需要用到数据可视化技术呢？在数据可视化技术中，你都用过哪些工具，你觉得哪个工具使用起来比较顺手？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-reproduction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
