{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "architectural-shield",
   "metadata": {},
   "source": [
    "分类回归树\n",
    "====\n",
    "CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作**分类树，又可以作回归树**。\n",
    "\n",
    "### 分類樹\n",
    "* 如果我构造了一棵决策树，想要基于**数据判断**这个人的职业身份，这个就属于**分类树**，因为是从几个分类中来做选择。\n",
    "* 处理**离散数据**，也就是数据种类**有限的数据**，它输出的是**样本的类别**\n",
    "\n",
    "### 回歸樹\n",
    "* 如果是给定了数据，想要**预测**这个人的年龄，那就属于**回归树**。\n",
    "* 对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个**数值**。\n",
    "\n",
    "## 1. CART 分类树的工作流程\n",
    "CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。基尼係數用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。\n",
    "\n",
    "基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个**不确定度降低的过程**，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择**基尼系数最小的属性**作为属性的划分。\n",
    "<img src=\"./images/18-02.png\">\n",
    "\n",
    "**p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。**\n",
    "\n",
    "* C為Choice的簡寫，(選擇該選項的個數)/(總筆數) = Ck/t，例如：該集合中總共6筆資料，其中選擇打球、不打球的人數分別為4與2，則C1=4, C2=2, t為6。\n",
    "\n",
    "通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：\n",
    "\n",
    "||集合|Gini|樣本穩定性|\n",
    "|----|:----|:----|:----:|\n",
    "|1|6 个都去打篮球|所有人都去打篮球，所以 p(Ck\\|t)=1，因此 GINI(t)=1-1=0。|較小，穩定|\n",
    "|2|3 个去打篮球，3 个不去打篮球|有一半人去打篮球，而另一半不去打篮球，所以，p(C1\\|t)=0.5，p(C2\\|t)=0.5，GINI(t)=1-（0.5\\*0.5+0.5\\*0.5）=0.5|較大，不穩定性更大|\n",
    "\n",
    "在 CART 算法中，基于基尼系数对**特征属性**进行**二元分裂**，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：\n",
    "<img src=\"./images/18-03.jpg\">\n",
    "\n",
    "节点 D 的基尼系数等于子节点 D1 和 D2 的**归一化基尼系数之和**，用公式表示为：\n",
    "<img src=\"./images/18-04.png\">\n",
    "\n",
    "归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到：\n",
    "* GINI(D1) = 0\n",
    "* GINI(D2) = 0.5\n",
    "\n",
    "所以在属性 A 的划分下，节点 D 的基尼系数为：\n",
    "* GINI(D,A) = 6/12\\*GINI(D1) + 6/12\\*GINI(D2) = 0.25\n",
    "\n",
    "节点 D 被属性 A 划分后的**基尼系数越大**，样本集合的**不确定性越大**，也就是**不纯度越高**。\n",
    "\n",
    "\n",
    "### 如何使用 CART 算法来创建分类树\n",
    "1. CART 分类树实际上是基于基尼系数来做**属性划分**的。\n",
    "2. 在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 **DecisionTreeClassifier** 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。\n",
    "\n",
    "下面，我们来用 CART 分类树，给 iris 数据集构造一棵分类决策树。在 sklearn 中也自带了这个数据集。基于 iris 数据集，构造 CART 分类树的代码如下：\n",
    "```python\n",
    "\n",
    "# encoding=utf-8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "# 准备数据集\n",
    "iris=load_iris()\n",
    "# 获取特征集和分类标识\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)\n",
    "# 创建CART分类树\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "# 拟合构造CART分类树\n",
    "clf = clf.fit(train_features, train_labels)\n",
    "# 用CART分类树做预测\n",
    "test_predict = clf.predict(test_features)\n",
    "# 预测结果与测试集结果作比对\n",
    "score = accuracy_score(test_labels, test_predict)\n",
    "print(\"CART分类树准确率 %.4lf\" % score)\n",
    "```\n",
    "運作結果：\n",
    "```python\n",
    "CART分类树准确率 0.9600\n",
    "```\n",
    "\n",
    "如果我们把决策树画出来，可以得到下面的图示：\n",
    "<img src=\"./images/18-05.png\">\n",
    "\n",
    "* 首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。\n",
    "* 使用 clf = DecisionTreeClassifier(criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。\n",
    "* 使用 clf.fit(train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。\n",
    "* 使用 clf.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。\n",
    "* 最后使用 accuracy_score(test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。\n",
    "* 我们能看到 sklearn 帮我们做了 CART 分类树的使用封装，使用起来还是很方便的。\n",
    "\n",
    "## 2. CART 回歸樹的工作流程\n",
    "### 離算程度，樣本x與均值μ之差的絕對值。\n",
    "**分類樹**與**回歸樹**的過程一樣，但**回歸樹**得到的預測結果為**連續的**，而且判斷**不純度**的指標不同。分類樹用**基尼係數**判斷不純度，回歸樹用**離散程度**即資料的混亂程度判斷不純度。\n",
    "樣本x減去樣本平均數，其值為**最小絕對偏差(LAD)**: \n",
    "<img src=\"./images/18-06.png\">\n",
    "**變異數(標準差的平方)** 公式，較常用**最小二乘偏差**:\n",
    "<img src=\"./images/18-07.png\">\n",
    "\n",
    "### 如何使用CART回歸樹做預測\n",
    "使用Sklearn的**波士頓房價數據集**，包含影響房價的指標有**犯罪率、房產稅等**，結果為房價。\n",
    "```python\n",
    "# encoding=utf-8\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 准备数据集\n",
    "boston=load_boston()\n",
    "# 探索数据\n",
    "print(boston.feature_names)\n",
    "# 获取特征集和房价\n",
    "features = boston.data\n",
    "prices = boston.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集\n",
    "train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)\n",
    "# 创建CART回归树\n",
    "dtr=DecisionTreeRegressor()\n",
    "# 拟合构造CART回归树\n",
    "dtr.fit(train_features, train_price)\n",
    "# 预测测试集中的房价\n",
    "predict_price = dtr.predict(test_features)\n",
    "# 测试集的结果评价\n",
    "print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))\n",
    "print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) \n",
    "```\n",
    "運行結果: \n",
    "```python\n",
    "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']\n",
    "回归树二乘偏差均值: 23.80784431137724\n",
    "回归树绝对值偏差均值: 3.040119760479042\n",
    "```\n",
    "回歸樹畫出來如下圖示，因數據集指標有些多，所以樹比較大:\n",
    "<img src=\"./images/18-08.png\">\n",
    "\n",
    "1. 透過tran_test_split將數據集分為**測試集、訓練集**。\n",
    "2. 使用dtr = DecisionTreeRegression()**初始化**一棵CART回歸樹\n",
    "3. 使用dtr.fit(train_features, train_price)函數，將**訓練集的特徵值**和**結果**作為參數進行擬合，得到回歸樹。\n",
    "4. 使用dtr.predict(test_features)函數進行預測，傳入**測試集的特徵值**，可以得到預測結果predict_price。\n",
    "5. 用mean_squared_error()求二乘偏差均值(變異數)、mean_abdolute_error()求絕對值偏差均值。\n",
    "\n",
    "## 3. 決策樹的剪枝\n",
    "CCP方法，為**後剪枝**方法，全稱cost-complexity prun**代價複雜度**。使用指標叫做**節點的表面誤差率增益值**。以作為**剪枝後誤差**的定義: \n",
    "<img src=\"./images/18-09.png\">\n",
    "1. Tt為以t為根節點的子樹，C(Tt)表示節點t的子樹**沒被裁剪時Tt的誤差**，C(t)表示節點t的子樹被**剪枝後節點t的誤差**，|Tt|代子樹Tt的葉子數，剪枝後，T的葉子數減少了|Tt|-1。\n",
    "2. 「節點的表面誤差率增益值」等於節點t的子樹被剪枝後的誤差變化除以剪掉的葉子數量。\n",
    "3. 希望剪枝後誤差最小，故尋找最小小α(alpha)值對應的節點，把它剪掉。這時生成第一個子樹，重複上面的過程，繼續剪枝，直到最後只剩下根節點，即最後一個子樹。\n",
    "4. 得到剪枝後子樹集合後，需要驗證最所有子樹的誤差計算一遍。可以通過計算每個子樹的誤差計算一遍。可以通過計算每個子樹的**基尼指數**或者平方誤差，取**誤差最小**的那個樹，得到我們想要的結果。\n",
    "\n",
    "## 總結\n",
    "今天我給你講了CART決策樹，它是一棵決策二叉樹，既可以做分類樹，也可以做回歸樹。\n",
    "1. 分類樹，CART採用基尼係數作為分段劃分的依據，得到的是離散的結果，也就是分類結果\n",
    "2. 回歸樹，CART可以採用最小絕對偏差（LAD），或者最小二乘偏差（LSD）作為劃分的依據，得到的是連續值，即回歸預測結果。\n",
    "\n",
    "三種決策樹之間的屬性選擇標准上的差異：\n",
    "* ID3算法，基於信息增益做判斷； \n",
    "* C4.5算法，基於信息增益率做判斷； \n",
    "* CART算法，分類樹是基於基尼係數實際上，這三個指標也是計算“不純度”的三種計算方式。在工具使用上，我們可以使用sklearn中的DecisionTreeClassifier創建CART分類樹，通過DecisionTreeRegressor創建CART回歸樹。你可以用代碼自己跑一遍我在演示文稿中舉到的例子。\n",
    "<img src=\"./images/18-10.png\">\n",
    "\n",
    "\n",
    "思考題\n",
    "====\n",
    "1. 你能說下ID3，C4.5，以及CART分類樹在做分區劃分時的區別嗎？\n",
    "2. 第二個問題是，sklearn中有個手寫數字數據集，調用的 方法是load_digits（），您能否創建一個CART分類樹，對手寫數字數據集做分類？另外選擇一部分測試集，統計下分類樹的準確率？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "robust-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART分类树准确率 0.9600\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "# 准备数据集\n",
    "iris=load_iris()\n",
    "# 获取特征集和分类标识\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集，返回三個array，訓練特徵值、測試特徵值、訓練標籤\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)\n",
    "# 创建CART分类树\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "# 拟合构造CART分类树\n",
    "clf = clf.fit(train_features, train_labels)\n",
    "\n",
    "# 用CART分类树做预测\n",
    "test_predict = clf.predict(test_features)\n",
    "\n",
    "# 预测结果与测试集结果作比对\n",
    "score = accuracy_score(test_labels, test_predict)\n",
    "print(\"CART分类树准确率 %.4lf\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "composed-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Bunch in module sklearn.utils object:\n",
      "\n",
      "class Bunch(builtins.dict)\n",
      " |  Bunch(**kwargs)\n",
      " |  \n",
      " |  Container object exposing keys as attributes.\n",
      " |  \n",
      " |  Bunch objects are sometimes used as an output for functions and methods.\n",
      " |  They extend dictionaries by enabling values to be accessed by key,\n",
      " |  `bunch[\"value_key\"]`, or by an attribute, `bunch.value_key`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> b = Bunch(a=1, b=2)\n",
      " |  >>> b['b']\n",
      " |  2\n",
      " |  >>> b.b\n",
      " |  2\n",
      " |  >>> b.a = 3\n",
      " |  >>> b['a']\n",
      " |  3\n",
      " |  >>> b.c = 6\n",
      " |  >>> b['c']\n",
      " |  6\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Bunch\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, key)\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "republican-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "回归树二乘偏差均值: 189.97898203592814\n",
      "回归树绝对值偏差均值: 10.373053892215568\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 准备数据集\n",
    "boston=load_boston()\n",
    "# 探索数据\n",
    "print(boston.feature_names)\n",
    "# 获取特征集和房价\n",
    "features = boston.data\n",
    "prices = boston.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集\n",
    "train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)\n",
    "# 创建CART回归树\n",
    "dtr=DecisionTreeRegressor()\n",
    "# 拟合构造CART回归树\n",
    "dtr.fit(train_features, train_price)\n",
    "# 预测测试集中的房价\n",
    "predict_price = dtr.predict(test_features)\n",
    "# 测试集的结果评价\n",
    "print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))\n",
    "print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "floral-minister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.data[0]\n",
    "boston.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-template",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
