{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crucial-exemption",
   "metadata": {},
   "source": [
    "樸素貝葉斯：適合做『文本分類、情感分析、垃圾郵件識別』，也常用自然語言處理NLP的工具。\n",
    "\n",
    "sklearn機器學習包\n",
    "====\n",
    "全稱Scikit-learn提供三個樸素貝葉斯分類算法：\n",
    "1. 高斯樸素貝葉斯(GaussianNB)：特徵變量是『連續變量』，符合高斯分佈，例如：『身高、物體的長度』。\n",
    "\n",
    "\n",
    "2. 多項式樸素貝葉斯(MultinominalNB)：特徵變量是『離散變量』，符合多項分佈，在文檔分類中特徵變量體現在一個單詞出現的次數，或者是單詞的TF-IDF值等。\n",
    "\n",
    "    以『單詞為粒度』，會計算在某個文件中具體次數。\n",
    "    \n",
    "\n",
    "3. 伯努利樸素貝葉斯(BernouliNB)：特徵變量是boolean變量，符合1/0分佈，在文檔分類中特徵是單詞是否出現。\n",
    "\n",
    "    以『文件為粒度』，如果該單詞在某文件出現即為1，否則為0。\n",
    "\n",
    "||特徵變量|適合場景|\n",
    "|----|----|----|\n",
    "|高斯樸素貝葉斯|連續變量|自然界『身高、體重』|\n",
    "|多項式樸素貝葉斯|離散變量|『單詞出現次數』或TF-IDF值|\n",
    "|伯努利樸素貝葉斯|boolean值|『文件中是否出現』|\n",
    "\n",
    "TF-IDF值\n",
    "====\n",
    "評估『某詞語』對一個文件集或文檔褲中的一份文件的重要程度。\n",
    "### 詞頻TF(Term Frequency)：\n",
    "單詞在文檔中『出現次數』，認為單詞的重要性和出現次數成正比。\n",
    "### 逆向文檔頻率IDF(Inverse Document Frequency)：\n",
    "單詞在文當中『區分度』，出現次數愈少，愈能『通過該單詞和其他文檔分開』。IDF愈大，代表該單詞『區分度愈大』。\n",
    "### TF-IDF = TF * IDF：\n",
    "即找到『單詞出現次數多』，同時『很少出現在其他文檔』的詞作為分類。IDF分母中，單詞出現的文檔數+1是有些單詞可能不存在文檔中，避免分母為0，統一給單詞出現的文檔數都+1。\n",
    "\n",
    "TF = 單詞出現次數/該文檔的總**單詞數**\n",
    "\n",
    "IDF = log[文檔總數/(該單詞出現的**文檔數**+1)]\n",
    "\n",
    "舉例：一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。\n",
    "\n",
    "\"this\"計算TF-IDF：\n",
    "* TF = 20/1000 = 0.02\n",
    "* IDF = log[10/(10+1)] = -0.0414\n",
    "* TF-IDF=0.02\\*(-0.0414)=-8.28e-4。\n",
    "\n",
    "\"bayes\"計算TF-IDF：\n",
    "* TF = 5/1000 = 0.005\n",
    "* IDF = log([10/(2+1)] = 0.5229\n",
    "* TF-IDF = 0.005\\*0.5229=2.61e-3\n",
    "\n",
    "很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。\n",
    "\n",
    "使用sklearn的TfdVectorizer類計算TF-IDF向量的值：\n",
    "====\n",
    "取sklearn計算的對數log時，底數是e，不是10。\n",
    "```python\n",
    "TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n",
    "```\n",
    "創建時有兩個參數：\n",
    "1. stop_words停用詞：為list()，為節省空間、時間，告訴機器不用計算這些沒有用的詞。\n",
    "2. token_pattern規律規則：為『正則表達』，\n",
    "\n",
    "|參數表|作用|\n",
    "|----|----|\n",
    "|stop_words|自定義停用詞表，為列表List類型|\n",
    "|token_pattern|過濾規則，正則表達，如r\"(?u)\\b\\w+\\b\"|\n",
    "\n",
    "### 使用fit_transform計算，矩陣列出『每個單詞在每個文檔的TF-IDF值』。\n",
    "\n",
    "|方法表|作用|\n",
    "|----|----|\n",
    "|fit_transform(X)|擬合模型，並返回文本矩陣|\n",
    "\n",
    "擬合後可得到更多的『TF-IDF向量屬性』，比如可得到詞彙的『對應關係(dict類型)』和『向量的IDF值』，也可獲得『設置的停用詞stop_words』。\n",
    "\n",
    "|屬性表|作用|\n",
    "|----|----|\n",
    "|vocabulary_|詞彙表，dict型|\n",
    "|idf_|返回idf值|\n",
    "|stop_words_|返回停用詞表|\n",
    "\n",
    "### 舉例 假設有四個文檔\n",
    "* 文檔一：this is the bayes documents\n",
    "* 文檔二：this is the second second documents\n",
    "* 文檔三：and the third one\n",
    "* 文檔四：is this the document\n",
    "\n",
    "計算文檔有『哪些單詞』，這些單詞在不同文檔中的『TF-IDF值』是多少。\n",
    "\n",
    "1. 首先創建TfidfVectorizer類：\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "```\n",
    "\n",
    "2. 創建4個文檔的列表documents，並讓創建好的tfidf_vec對documents進行擬合，得到TF-IDF矩陣：\n",
    "```python\n",
    "documents = [\n",
    "    'this is the bayes document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n",
    "```\n",
    "輸出文檔中所有不重複的詞：\n",
    "```python\n",
    "print('不重复的词:', tfidf_vec.get_feature_names())\n",
    "```\n",
    "運行結果：\n",
    "```python\n",
    "不重复的词: ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "```\n",
    "輸出每個單詞在每個文檔中的TF-IDF，向量裡的順序是按照詞語的id順序：\n",
    "```python\n",
    "print('每个单词的tfidf值:', tfidf_matrix.toarray())\n",
    "```\n",
    "運行結果：\n",
    "```python\n",
    "每个单词的tfidf值: [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n",
    "  0.33040189 0.         0.40412895]\n",
    " [0.         0.         0.27230147 0.27230147 0.         0.85322574\n",
    "  0.22262429 0.         0.27230147]\n",
    " [0.55280532 0.         0.         0.         0.55280532 0.\n",
    "  0.28847675 0.55280532 0.        ]\n",
    " [0.         0.         0.52210862 0.52210862 0.         0.\n",
    "  0.42685801 0.         0.52210862]]\n",
    "```\n",
    "\n",
    "## 如何最文檔進行分類：\n",
    "想個重要階段：\n",
    "1. 基於分詞的數據準備，包括分詞、單詞權重計算、去掉停用詞\n",
    "2. 應用樸素貝葉斯分類進行分類，首先通過訓練集得到樸素貝葉斯分類器，然後將分類器應用於測試集，並與實際結果做對比，最終得到測試集的分類準確率。\n",
    "<img src=\"./images/21-01.jpg\">\n",
    "\n",
    "## 模塊1: 對文檔進行分詞\n",
    "* 在英文文档中，最常用的是 NTLK 包。NTLK 包中包含了英文的停用词 stop words、分词和标注方法。\n",
    "```python\n",
    "import nltk\n",
    "word_list = nltk.word_tokenize(text) #分词\n",
    "nltk.pos_tag(word_list) #标注单词的词性\n",
    "```\n",
    "* 在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。\n",
    "```python\n",
    "import jieba\n",
    "word_list = jieba.cut (text) #中文分词\n",
    "```\n",
    "\n",
    "## 模塊2: 加載停用詞表\n",
    "我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。\n",
    "```python\n",
    "stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]\n",
    "```\n",
    "\n",
    "## 模塊3: 計算單詞的權重\n",
    "这里我们用到 sklearn 里的 TfidfVectorizer 类，上面我们介绍过它使用的方法。直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。\n",
    "```python\n",
    "tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "features = tf.fit_transform(train_contents)\n",
    "```\n",
    "这里 max_df 参数用来描述单词在文档中的最高出现率。**假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了**，那么它只携带了『非常少的信息』，因此就不作为分词统计。一般很少设置 min_df，因为 min_df 通常都会很小。\n",
    "\n",
    "## 模塊4: 生成樸素貝葉斯分類器\n",
    "1. 将特征『训练集的特征空间 train_features』，以及『训练集对应的分类 train_labels』传递给『贝叶斯分类器 clf』，它会自动生成一个符合特征空间和对应分类的分类器。\n",
    "2. **多项式贝叶斯分类器**，其中 alpha 为『平滑参数』。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的『抽样情况』，我们**不能因为一个事件没有观察到，就认为整个事件的概率为 0**。为了解决这个问题，我们需要做平滑处理。\n",
    "3. 当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用**加 1** 的方式，来统计**没有出现过的单词的概率**。这样当训练样本很大的时候，加 1 得到的概率变化**可以忽略不计**，也同时避免了零概率的问题。\n",
    "4. 当 0<alpha<1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001\n",
    "```python\n",
    "# 多项式贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n",
    "```\n",
    "\n",
    "## 模塊5: 使用生成的分類器做預測\n",
    "1. 須先得到測試集的特徵矩陣，用**訓練集的分詞**創建TfidfVectorizer類，使用**同樣的stop_words和max_df**, 然後用這個TfidfVectorizer類對測試集的內容進行fit_transform擬合，得到測試集的特徵矩陣test_features。\n",
    "```python\n",
    "test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\n",
    "test_features=test_tf.fit_transform(test_contents)\n",
    "```\n",
    "\n",
    "2. 用訓練好的分類器最新數據做預測，**使用predict函數**，傳入測試集的特徵矩陣test_features，得到**分类结果 predicted_labels**。predict 函数做的工作就是求解所有**后验概率**并找出最大的那个。\n",
    "```python\n",
    "predicted_labels=clf.predict(test_features)\n",
    "```\n",
    "\n",
    "## 模塊6: 計算準確率\n",
    "计算准确率实际上是对分类模型的评估。我们可以调用 sklearn 中的 metrics 包，在 **metrics 中提供了 accuracy_score 函数**，方便我们对实际结果和预测的结果做对比，给出模型的**准确率**。使用方法如下：\n",
    "```python\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(test_labels, predicted_labels)\n",
    "```\n",
    "\n",
    "## 數據挖掘神器sklearn\n",
    "* 从数据挖掘的流程来看，一般包括了**获取数据、数据清洗、模型训练、模型评估和模型部署**这几个过程。\n",
    "* sklearn 中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。\n",
    "* 在文档分类的项目中，我们针对文档的特点，给出了基于分词的准备流程。一般来说 **NTLK 包适用于英文文档**，而 **jieba 适用于中文文档**。我们可以根据文档选择不同的包，对文档提取分词。这些分词就是贝叶斯分类中最重要的特征属性。基于这些分词，我们得到**分词的权重**，即特征矩阵。\n",
    "* 通过特征矩阵与分类结果，我们就可以创建出朴素贝叶斯分类器，然后用**分类器进行预测**，最后预测结果与实际结果做对比即可以得到分类器在测试集上的准确率。\n",
    "<img src=\"./images/21-02.png\">\n",
    "\n",
    "練習題\n",
    "====\n",
    "我已经讲了中文文档分类中的 6 个关键的模块，最后，我给你留一道对中文文档分类的练习题吧。\n",
    "* 數據說明：\n",
    "    1. 文档共有 4 种类型：女性、体育、文学、校园；\n",
    "    2. 训练集放到 train 文件夹里，测试集放到 test 文件夹里，停用词放到 stop 文件夹里。\n",
    "    \n",
    "* 请使用朴素贝叶斯分类对训练集进行训练，并对测试集进行验证，并给出测试集的准确率。\n",
    "* 最后你不妨思考一下，假设我们要判断一个人的性别，是通过身高、体重、鞋码、外貌等属性进行判断的，如果我们用朴素贝叶斯做分类，适合使用哪种朴素贝叶斯分类器？停用词的作用又是什么？\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romance-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "outstanding-leisure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的词: ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    'this is the bayes document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n",
    "\n",
    "print('不重复的词:', tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "threaded-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的ID: {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "print('每个单词的ID:', tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "duplicate-solomon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的tfidf值: [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.         0.         0.27230147 0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.         0.52210862 0.52210862 0.         0.\n",
      "  0.42685801 0.         0.52210862]]\n"
     ]
    }
   ],
   "source": [
    "print('每个单词的tfidf值:', tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-parts",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
